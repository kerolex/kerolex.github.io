<!DOCTYPE html>
<html lang = "en">

  <head>
    <title>Alessio Xompero</title>

    <link href = "css/bootstrap.min.css" rel = "stylesheet">
    <link rel="stylesheet" href="mystyle.css">

    <style>
      h2{
        text-align: left;
        color: #017572;
      }
      
      .bib {
        width: 50%;
        padding: 10px 0px 0px 20px;
        text-align: left;
        background-color: rgba(128, 128, 128, 0);
        margin-top: 0px;
        white-space: pre;
      }
	 
	 <!---
      #bib_morb, #bibdiv, #bib_fusion, #bib_lanzicassp {
        width: 50%;
        padding: 10px 0px 0px 20px;
        text-align: left;
        background-color: rgba(128, 128, 128, 0);
        margin-top: 0px;
        white-space: pre;
      }
      -->

      .abstract {
        width: 50%;
        padding: 10px 0px 0px 20px;
        font-style: italic;
        text-align: left;
        background-color: rgba(128, 128, 128, 0);
        margin-top: 0px;
        white-space: break-all;
      }
      
	#pubslist {
		width: 50%;
		text-align: left;
		margin-top: 0px;
		margin:0 auto;
		white-space: break-all;
	}
    </style>

    <script> 
      function AppearBib(bib_id) {
        var x = document.getElementById(bib_id);
        if (x.style.display === "none") {
          x.style.display = "block";
        } else {
          x.style.display = "none";
        }
      }

      function AppearAbstract(abstract_id) {
        var x = document.getElementById(abstract_id);
        if (x.style.display === "none") {
          x.style.display = "block";
        } else {
          x.style.display = "none";
        }
      }
    </script>
  </head>

<body>
	<nav style="text-align:center">
		<ul>
			<li><a href="index.html">Alessio Xompero</a></li>
			<li><a href="index.html">Home</a></li>
			<li><a href="publications.html" class="active">Publications</a></li>
			<li><a href="bio.html">Biography</a></li>
		</ul>
	</nav>


  <div id="pubslist">
  
  <h2 style="text-align: center;">2019</h2>
  <h3>Conferences </h3>
  <ul>
  <li>
  <div>
  <em>Accurate target annotation in 3D from multimodal streams</em><br>
  O. Lanz, A. Brutti, <strong>A. Xompero</strong>, X. Qian, M. Omologo, and A. Cavallaro<br>
  International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton (UK), May, 12-17, 2019<br>
  <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_lanzicassp')">Abstract</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="">PDF</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="">Slides</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Cite" onclick="AppearBib('bib_lanzcassp')">Bibtex</a>
  </div>
  <div id="bib_lanzicassp"  style="display:none;" class="bib">
  @INPROCEEDINGS{Lanz20189ICASSP,
  title = {Accurate target annotation in 3D from multimodal streams},
  author = {Lanz, Oswald and Brutti, Alessio, and Xompero, Alessio and Xinyuan, Qian and Omologo, Maurizion and Cavallaro, Andrea},
  booktitle = ICASSP,
  address = {Brighton, UK},
  month = "12--17~" # may,
  year = {2019}
  }
  </div>
  <div id="abstract_lanzicassp"  style="display:none;" class="abstract">
  Accurate  ground-truth annotation is fundamental for quantifying the 
  performance of multi-sensor and multi-modal object detectors and trackers. 
  However, these scenarios require invasive or expensive instrumentation 
  to automatically generate a ground-truth annotation. To mitigate this 
  problem, we present a multi-modal approach that leverages annotations 
  from reference streams (e.g. individual camera views) and measurements 
  from non-annotated additional streams (e.g.~audio) to infer 3D trajectories 
  through an optimization. The core of our approach is a multi-modal 
  extension of Bundle Adjustment with a cross-modal correspondence 
  detection to selectively use measurements in the optimization. 
  We apply this approach to fully annotate a new multi-modal and 
  multi-view dataset for 3D tracking of multiple speakers. 
  </div>
  </li>	 
  </ul>
  
   <hr />
  <h2 style="text-align: center;">2018</h2>
  <h3>Conferences </h3>
  <ul>
  <li>
  <div>
  <em>MORB: a multi-scale binary descriptor</em> <br>
  <strong>A. Xompero</strong>, O. Lanz, and A. Cavallaro<br> 
  IEEE International Conference on Image Processing, Athens (Greece), October, 7-10, 2018<br>
  <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_morb')">Abstract</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="projects/icip2018/2018_ICIP_MORBaMultiScaleBinaryDescriptor_Xompero_Lanz_Cavallaro.pdf">PDF</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Downloadr" href="projects/icip2018/2018_ICIP_poster_Xompero_Lanz_Cavallaro.pdf">Poster</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Cite bib" onclick="AppearBib('bib_morb')">Bibtex</a>
  </div>
  <div id="abstract_morb" style="display:none;" class="abstract">
  Local  image  features  play  an  important  role  in  matching  images
  under different geometric and photometric transformations.   How-
  ever,  as  the  scale  difference  across  views  increases,  the  matching
  performance may considerably decrease.   To address this problem
  we propose MORB, a multi-scale binary descriptor that is based on
  ORB and that improves the accuracy of feature matching under scale
  changes.  MORB describes an image patch at different scales using
  an  oriented  sampling  pattern  of  intensity  comparisons  in  a  prede-
  fined set of pixel pairs.  We also propose a matching strategy that
  estimates the cross-scale match between MORB descriptors across
  views.  Experiments show that MORB outperforms state-of-the-art
  binary descriptors under several transformations.
  </div>
  <div id="bib_morb"  style="display:none;" class="bib">@INPROCEEDINGS{Xompero2018ICIP_MORB,
  title = {{MORB: a multi-scale binary descriptor}},
  author = {Xompero, Alessio and Lanz, Oswald and Cavallaro, Andrea},
  booktitle = ICIP,
  address = {Athens, Greece},
  month = "7--10~" # oct,
  year = {2018}
  }
  </div>
  </li>
  <br>
  <li>
  <div>
  <em>Multi-camera Matching of Spatio-Temporal Binary Features</em><br>
  <strong>A. Xompero</strong>, O. Lanz, and A. Cavallaro<br>
  International Conference on Information Fusion, Cambridge (United Kingdom), July, 10-13, 2018. <br>
  <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_fusion')">Abstract</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="projects/fusion2018/2018_FUSION_MultiCameraMatchingOfSpatioTemporalBinaryFeatures_Xompero_Lanz_Cavallaro.pdf">PDF</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="http://cis.eecs.qmul.ac.uk/2018SummerSchool/AlessioXompero_CIS_SummerSchool2018.pdf">Slides</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Cite bib" onclick="AppearBib('bib_fusion')">Bibtex</a>
  </div>
  <div id="bib_fusion"  style="display:none;" class="bib">
  @INPROCEEDINGS{Xompero2018FUSION,
  title = {{Multi-camera Matching of Spatio-Temporal Binary Features}},
  author = {Xompero, Alessio and Lanz, Oswald and Cavallaro, Andrea},
  booktitle = FUSION,
  address = {Cambridge, UK},
  month = "10--13~" # jul,
  year = {2018}
  }
  </div>
  <div id="abstract_fusion"  style="display:none;" class="abstract">
	Local  image  features  are  generally  robust  to  different  geometric
	and  photometric  transformations  on  planar surfaces or under narrow  
	baseline  views.  However,  the  matching  performance  decreases  
	considerably  across  cameras  with unknown  poses  separated  by  a
	wide  baseline.  To  address  this problem, we accumulate temporal 
	information within each view by  tracking  local  binary  features,  
	which  encode  intensity  comparisons  of  pixel  pairs  in  an  image
	patch.  We  then  encode  the spatio-temporal  features  into  fixed-length
	binary  descriptors  by selecting  temporally  dominant  binary  values.  
	We  complement the   descriptor   with   a   binary   vector   that   identifies   intensity
	comparisons  that  are  temporally  unstable.  Finally,  we  use  this
	additional vector to ignore the corresponding binary values in the
	fixed-length binary descriptor when matching the features across
	cameras. We analyse the performance of the proposed approachand  compare  it  with  baselines. 
  </div>
  </li>
  <br>
  <li>
  <div>
  <em>3D Mouth Tracking from a Compact Microphone Array Co-located with a Camera</em><br>
  X. Qian, <strong>A. Xompero</strong>, A. Brutti, O. Lanz, M. Omologo, and A. Cavallaro<br>
  International Conference on Acoustics, Speech and Signal Processing (ICASSP), Calgary (Canada), April, 15-20, 2018<br>
  <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_qianicassp')">Abstract</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="http://www.eecs.qmul.ac.uk/~andrea/papers/2018_ICASSP_3DMouthTracking_Qian_Xompero_Brutti_Lanz_Omologo_Cavallaro.pdf">PDF</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="">Poster</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Cite" onclick="AppearBib('bib_qianicassp')">Bibtex</a>
  </div>
  <div id="bib_qianicassp"  style="display:none;" class="bib">
  @INPROCEEDINGS{Qian2018ICASSP,
  title = {{3D Mouth Tracking from a Compact Microphone Array Co-located with a Camera}},
  author = {Qian, Xinyuan and Xompero, Alessio and Lanz, Oswald and Cavallaro, Andrea},
  booktitle = ICASSP,
  address = {Calgari, Canada},
  month = "15--20~" # apr,
  year = {2018}
  }
  </div>
  <div id="abstract_qianicassp"  style="display:none;" class="abstract">
  We address the 3D audio-visual mouth tracking problem when using
  a compact platform with co-located audio-visual sensors, without a
  depth camera.  In particular, we propose a multi-modal particle fil-
  ter that combines a face detector and 3D hypothesis mapping to the
  image plane. The audio likelihood computation is assisted by video,
  which relies on a GCC-PHAT based acoustic map.  By combining
  audio and video inputs, the proposed approach can cope with a re-
  verberant and noisy environment, and can deal with situations when
  the person is occluded, outside the Field of View (FoV), or not facing
  the sensors.  Experimental results show that the proposed tracker is
  accurate both in 3D and on the image plane. 
  </div>
  </li>
  </ul>

  <hr />
  <h2 style="text-align: center;">2016</h2>
  <h3>Journals</h3>
  <ul>
  <li>
  <div>
  <em>An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes</em><br>
  S. Ba, <span class="papercite_highlight">X. Alameda-Pineda</span>, <strong>A. Xompero</strong>, and R. Horaud<br>
  Computer Vision and Image Understanding, 2016<br>
  <a class="papercite_pdf" title="Download" href="http://xavirema.eu/wp-content/papercite-data/pdf/Ba-CVIU-2016.pdf">PDF</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="arXiv" href="http://arxiv.org/abs/1509.01520">arXiv</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Cite" onclick="AppearBib('bib_bacviu')">Bibtex</a>
  </div>
  <div id="bib_bacviu"  style="display:none;" class="bib">
  @ARTICLES{bA2016cviu,
  title = {An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes},
  author = {Ba, S. and Alameda-Pineda, X. and Xompero, A. and Horaud, R.},
  journal = {Computer Vision and Image Understanding},
  vol={153},
  num={},
  pages={64--76},
  month = apr,
  year = {2016},
  }
  </div>
  <div id="abstract_bacviu"  style="display:none;" class="abstract">
   Object tracking is an ubiquitous problem that appears in many applications
   such as remote sensing, audio processing, computer vision, human-machine
   interfaces, human-robot interaction, etc.  Although thoroughly investigated
   in  computer  vision,  tracking  a  time-varying  number  of  persons  remains  a
   challenging open problem.  In this paper, we propose an on-line variational
   Bayesian model for multi-person tracking from cluttered visual observations
   provided  by  person  detectors.   The  paper  has  the  following  contributions.
   We propose a variational Bayesian framework for tracking an unknown and
   varying number of persons.  Our model results in a variational expectation-maximization  (VEM)
   algorithm  with  closed-form  expressions  both  for  the
   posterior distributions of the latent variables and for the estimation of the
   model parameters.  The proposed model exploits observations from multiple
   detectors,  and  it  is  therefore  multimodal  by  nature.   Finally,  we  propose
   to  embed  both  object-birth  and  object-visibility  processes  in  an  effort  to
   robustly  handle  temporal  appearances  and  disappearances.   Evaluated  on
   classical  multiple  person  tracking  datasets,  our  method  shows  competitive
   results with respect to state-of-the-art multiple-object tracking algorithms, 
   such as the probability hypothesis density (PHD) filter, among others.
  </div>
  </li>
  </ul>

  <hr />

  <h2 style="text-align: center;">Others</h2>
  
  I am also appearing in the acknowledgments of the following publications for my collaboration/help:
  <ul>
  <li>
	  <em>Detecting group formations using iBeacon technology</em><br>
	  Kleomenis K., Hamed H., Laurissa T., and Richard G. C.<br>
	  Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct (UbiComp '16)<br>
	  <a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/14410/Tokarchuk%20Detecting%20Group%20Formations%202016%20Accepted.pdf?sequence=1">PDF</a>
  </li>
  <li>
	  <em>Synchronization of Multi-User Event Media at MediaEval 2015: Task Description, Datasets, and Evaluation</em><br>
	  Conci, N., De Natale, F.G. and Mezaris, V.<br>
	  MediaEval 2015 Workshop, Wurzen, Germany, September, 2015<br>
	  <a href="https://www.semanticscholar.org/paper/Synchronization-of-Multi-User-Event-Media-at-Conci-Mezaris/3398aaea553389930485b0812570ed8ce2722e62/pdf">PDF</a>
  </li>
  <li>
	  <em>Synchronization of Multi-User Event Media (SEM) at MediaEval 2014: Task Description, Datasets, and Evaluation</em><br>
	  Conci, N., De Natale, F., Mezaris, V. and Matton, M.<br>
	  MediaEval 2014 October, 2014<br>
	  <a href="https://www.semanticscholar.org/paper/Synchronization-of-Multi-User-Event-Media-SEM-at-Conci-Natale/9c1929db34b53304baf48278a8a99c70c12b495c/pdf">PDF</a>
  </li>
  </ul>
 </div>
</body>
</html>


