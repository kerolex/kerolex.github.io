<!DOCTYPE html>
<html lang = "en">

<head>
 <!-- Global site tag (gtag.js) - Google Analytics -->
 <script async src="https://www.googletagmanager.com/gtag/js?id=UA-139569814-1"></script>
 <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-139569814-1');
</script>

<title>Alessio Xompero</title>

<link href = "css/bootstrap.min.css" rel = "stylesheet">
<link rel="stylesheet" href="mystyle.css">

<style>
  h2{
    text-align: left;
    color: #017572;
  }

  .bib {
    width: 50%;
    padding: 10px 0px 0px 20px;
    text-align: left;
    background-color: rgba(128, 128, 128, 0);
    margin-top: 0px;
    white-space: pre;
  }

  <!---
  #bib_morb, #bibdiv, #bib_fusion, #bib_lanzicassp {
    width: 50%;
    padding: 10px 0px 0px 20px;
    text-align: left;
    background-color: rgba(128, 128, 128, 0);
    margin-top: 0px;
    white-space: pre;
  }
  -->

  .abstract {
    width: 100%;
    padding: 1px 0px 0px 20px;
    font-style: italic;
    text-align: left;
    background-color: rgba(128, 128, 128, 0);
    margin-top: 0px;
    white-space: break-all;
  }

  #pubslist {
    width: 50%;
    text-align: left;
    margin-top: 0px;
    margin:0 auto;
    white-space: break-all;
  }
</style>

<script> 
  function AppearBib(bib_id) {
    var x = document.getElementById(bib_id);
    if (x.style.display === "none") {
      x.style.display = "block";
    } else {
      x.style.display = "none";
    }
  }

  function AppearAbstract(abstract_id) {
    var x = document.getElementById(abstract_id);
    if (x.style.display === "none") {
      x.style.display = "block";
    } else {
      x.style.display = "none";
    }
  }
</script>
</head>

<body>
	<nav style="text-align:center">
		<ul>
			<li><a href="index.html">Alessio Xompero</a></li>
			<li><a href="index.html">Home</a></li>
			<li><a href="publications.html" class="active">Publications</a></li>
			<li><a href="bio.html">Biography</a></li>
		</ul>
	</nav>


  <div id="pubslist">
    <div>
      <h2 style="text-align: left;">2020</h2>
      <h3>Journals</h3>
      <div class="paper-teaser"><img src="images/tip20_mst.png" alt="[img]">
      </div>
      <div class="paper-data">
        <em style="color:#008000">A spatio-temporal multi-scale binary descriptor</em><br>
        <b style="font-weight: bold;">A. Xompero</b>, O. Lanz, and A. Cavallaro<br>
        Accepted in IEEE Transactions on Image Processing<br>
        <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_xomperotip')">Abstract</a>
        &nbsp | &nbsp
        <a class="papercite_pdf" title="Cite" onclick="AppearBib('bib_xomperotip')">Bibtex</a>
      </div>
      <div id="bib_xomperotip"  style="display:none;" class="bib">
       @misc{xompero2020TIP_MST,
       title={A spatio-temporal multi-scale binary descriptor},
       author={Alessio Xompero and Oswald Lanz and Andrea Cavallaro},
       journal={IEEE Transactions on Image Processing},
       volume={},
       number={},
       month={},
       year={2020}
     }
   </div>
   <p id="abstract_xomperotip"  style="display:none;" class="abstract">
    Binary descriptors are widely used for multi-view matching and robotic navigation. However, their matching performance decreases considerably under severe scale and viewpoint changes in non-planar scenes. To overcome this problem, we propose to encode the varying appearance of selected 3D scene points tracked by a moving camera with compact spatio-temporal descriptors. To this end, we first track interest points and capture their temporal variations at multiple scales. Then, we validate feature tracks through 3D reconstruction and compress the temporal sequence of descriptors by encoding the most frequent and stable binary values. Finally, we determine  multi-scale correspondences across views with a matching strategy that handles severe scale differences. The proposed spatio-temporal multi-scale approach is generic and can be used with a variety of binary descriptors. 
    We show the effectiveness of the joint multi-scale extraction and temporal reduction through comparisons of different temporal reduction strategies and the application to several binary descriptors.
  </p>
  <div class="paper-teaser">
    <img src="http://corsmal.eecs.qmul.ac.uk/benchmark/resources/handover.gif" alt="[Handover header]">
  </div>
  <div class="paper-data">
    <em style="color:#008000">Benchmark for Human-to-Robot Handovers of Unseen Containers with Unknown Filling</em><br>
    R. Sanchez-Matilla, K. Chatzilygeroudis, A. Modas, N. Ferreira Duarte, <b style="font-weight: bold;">A. Xompero</b>, P. Frossard, A. Billard, and A. Cavallaro<br>
    To appear in IEEE Robotics and Automation Letters<br>
    <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_sanchezral')">Abstract</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Download" href="">PDF</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Cite" onclick="AppearBib('bib_sanchezral')">Bibtex</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Link" href="http://corsmal.eecs.qmul.ac.uk/benchmark.html">Webpage</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Video" href="http://corsmal.eecs.qmul.ac.uk/benchmark/resources/Benchmark.mp4">Video</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Code" href="https://github.com/CORSMAL/Benchmark">Code</a>
  </div>
  <div id="bib_sanchezral"  style="display:none;" class="bib">
   @misc{SanchezMatilla2020RAL_Benchmark,
   title={Benchmark for Human-to-Robot Handovers of Unseen Containers with Unknown Filling},
   author={Ricardo Sanchez-Matilla, Konstantinos Chatzilygeroudis, Apostolos Modas, Nuno Ferreira Duarte, Alessio Xompero, Pascal Frossard, Aude Billard, and Andrea Cavallaro},
   journal={IEEE Robotics and Automation Letters},
   volume={},
   number={},
   month={},
   year={2020}
 }
</div>
<p id="abstract_sanchezral"  style="display:none;" class="abstract">
  The real-time estimation through vision of the physical properties of objects manipulated by humans is important to inform the control of robots for performing accurate and safe grasps of objects handed over by humans. However, estimating the 3D pose and dimensions of previously unseen objects using only RGB cameras is challenging due to illumination variations, reflective surfaces, transparencies, and occlusions caused both by the human and the robot. In this paper, we present a benchmark for dynamic human-to-robot handovers that do not rely on a motion capture system, markers, or prior knowledge of specific objects. To facilitate comparisons, the benchmark focuses on cups with different levels of transparencies and with an unknown amount of an unknown filling. The performance scores assess the overall system as well as its components in order to help isolate modules of the pipeline that need improvements. In addition to the task description and the performance scores, we also present and distribute as open source a baseline implementation for the overall pipeline to enable comparisons and facilitate progress.
</p>
<h3>Conferences </h3>
<div class="paper-teaser">
  <img src="images/diagram_ICASSP20.png" alt="[img]">
</div>
<div class="paper-data">
 <em style="color:#008000">Multi-view shape estimation of transparent containers</em><br>
 <b style="font-weight: bold;">A. Xompero</b>, R. Sanchez-Matilla, A. Modas, P. Frossard, and A. Cavallaro<br>
 Submitted to International Conference on Acoustics, Speech and Signal Processing (ICASSP)<br>
 <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_xomperoicassp')">Abstract</a>
 &nbsp | &nbsp
 <a class="papercite_pdf" title="Download" href="https://arxiv.org/pdf/1911.12354.pdf">PDF</a>
 &nbsp | &nbsp
 <a class="papercite_pdf" title="Cite" onclick="AppearBib('bib_xomperoicassp')">Bibtex</a>
 &nbsp | &nbsp
 <a class="papercite_pdf" title="Dataset" href="http://corsmal.eecs.qmul.ac.uk/CORSMAL_Containers.html">Data</a>
 &nbsp | &nbsp
 <a class="papercite_pdf" title="Link" href="http://corsmal.eecs.qmul.ac.uk/CORSMAL_Containers_LoDE.html">Webpage</a>
</div>
<div id="bib_xomperoicassp"  style="display:none;" class="bib">
 @misc{xompero2019multiview,
 title={Multi-view shape estimation of transparent containers},
 author={Alessio Xompero and Ricardo Sanchez-Matilla and Apostolos Modas and Pascal Frossard and Andrea Cavallaro},
 year={2019},
 eprint={1911.12354},
 archivePrefix={arXiv},
 primaryClass={cs.CV}
}
</div>
<div id="abstract_xomperoicassp"  style="display:none;" class="abstract">
  <p>
    The 3D localisation of an object and the estimation of its properties, such as shape and dimensions, are challenging 
    under varying degrees of transparency and lighting conditions. In this paper, we propose a method for jointly localising 
    container-like objects and estimating their dimensions using two wide-baseline, calibrated RGB cameras. Under the assumption 
    of vertical circular symmetry, we estimate the dimensions of an object by sampling at different heights a set of sparse 
    circumferences with iterative shape fitting and image re-projection to verify the sampling hypotheses in each camera using 
    semantic segmentation masks. We evaluate the proposed method on a novel dataset of objects with different degrees of 
    transparency and captured under different backgrounds and illumination conditions. Our method, which is based on RGB 
    images only outperforms, in terms of localisation success and dimension estimation accuracy a deep-learning based 
    approach that uses depth maps.  
  </p>
</div>
<br>
<h3 style="text-align:left;">PhD Thesis</h3>
<div>
  <div class="paper-teaser">
    <img src="images/phdthesis_main.jpg" alt="[img]">
  </div>
  <div class="paper-data">
    <em style="color:#008000">Local features for view matching across independently moving cameras</em> 
    <a class="papercite_pdf" title="Download" href="">PDF</a><br>
    Supervisors: Prof. Andrea Cavallaro (QMUL, UK), Dr. Oswald Lanz (FBK, IT)<br>
    Examiners: Dr. Stefan Leutenegger (Imperial College London, UK), Dr. Jean-Yves Guillemaut (University of Surrey, UK)<br>
    <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_phd')">Abstract</a>
  </div>
  <p id="abstract_phd"  style="display:none;" class="abstract">
    Matching views across independently moving cameras is important for 3D reconstruction and navigation.  However, severe geometric and photometric differences, such as viewpoint, scale, and illumination changes, can considerably decrease the matching performance. This thesis proposes novel, compact, local features that can cope with scale and viewpoint variations. We extract and describe an image patch at different scales of an image pyramid by comparing intensity values between learned pixel-pair locations, and employ a cross-scale distance when matching these features.
    We capture, at multiple scales, the temporal changes of a 3D point, as observed in the visual stream of a camera, by tracking local image binary descriptors.  After validating the feature-point trajectories through 3D reconstruction, we reduce, for each scale, the sequence of binary descriptors to a compact, fixed-length descriptor that identifies the most frequent and the most stable binary tests over time. We organise local spatio-temporal features extracted at a single scale in an incremental and adaptive ternary tree, which is stored locally within each camera. We then propose a novel decentralised approach that performs online cross-camera visual place recognition simultaneously to the formation of the tree, while the camera processes the video stream. Experiments show that the proposed approaches improve the accuracy when matching features across cameras, compared to existing approaches.
  </p>
</div>
</div>
<hr />
<div>
  <h2 style="text-align: left;">2019</h2>
  <h3>Conferences </h3>
</div>
<br>
<div class="paper-teaser">
	<img src="images/multisensor.png" alt="[img]">
</div>
<div class="paper-data">
 <em style="color:#008000">Accurate target annotation in 3D from multimodal streams</em><br>
 O. Lanz, A. Brutti, <b style="font-weight: bold;">A. Xompero</b>, X. Qian, M. Omologo, and A. Cavallaro<br>
 International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton (UK), May, 12-17, 2019<br>
 <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_lanzicassp')">Abstract</a>
 &nbsp | &nbsp
 <a class="papercite_pdf" title="Download" href="http://www.eecs.qmul.ac.uk/~andrea/papers/2019_ICASSP__AccurateTargetAnnotationIn3DFromMultimodalStreams_Lanz_Brutti_Xompero_Qian_Omologo_Cavallaro.pdf">PDF</a>
 &nbsp | &nbsp
 <a class="papercite_pdf" title="Cite" onclick="AppearBib('bib_lanzicassp')">Bibtex</a>
 &nbsp | &nbsp
 <a class="papercite_pdf" title="Dataset" href="https://ict.fbk.eu/units/speechtek/cav3d/">CAV3D</a>
</div>
<div id="bib_lanzicassp"  style="display:none;" class="bib">
  @INPROCEEDINGS{Lanz2019ICASSP,
  title = {Accurate target annotation in 3D from multimodal streams},
  author = {Lanz, Oswald and Brutti, Alessio, and Xompero, Alessio and Xinyuan, Qian and Omologo, Maurizio and Cavallaro, Andrea},
  booktitle = ICASSP,
  address = {Brighton, UK},
  month = "12--17~" # may,
  year = {2019}
}
</div>
<div id="abstract_lanzicassp"  style="display:none;" class="abstract">
  Accurate annotation  is  fundamental  to  quantify  the  performance of multi-sensor  and  multi-modal object detectors and trackers.
  However,  invasive  or  expensive  instrumentation  is  needed  to automatically  generate  these  annotations. To  mitigate this problem,
  we present a multi-modal approach that leverages annotations from reference streams (e.g. individual camera views) and measurements
  from  unannotated  additional  streams  (e.g.  audio)  to  infer  3D trajectories  through  an  optimization.   The  core  of  our  approach
  is a multi-modal  extension  of Bundle  Adjustment  with  a cross-modal correspondence detection that selectively uses measurements in the
  optimization. We apply the proposed approach to fully annotate a new multi-modal and multi-view dataset for multi-speaker 3D tracking. 
</div>
</div>
<br>

<div id="pubslist">
  <hr />
  <h2 style="text-align: left;">2018</h2>
  <h3 style="text-align: left;">Conferences </h3>
  <div class="paper-teaser">
   <img src="images/MORB.png" alt="[img]">
 </div>
 <div class="paper-data">	  
  <em style="color:#008000">MORB: a multi-scale binary descriptor</em> <br>
  <strong style="font-weight: bold;">A. Xompero</strong>, O. Lanz, and A. Cavallaro<br> 
  IEEE International Conference on Image Processing, Athens (Greece), October, 7-10, 2018<br>
  <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_morb')">Abstract</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="projects/icip2018/2018_ICIP_MORBaMultiScaleBinaryDescriptor_Xompero_Lanz_Cavallaro.pdf">PDF</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="projects/icip2018/2018_ICIP_poster_Xompero_Lanz_Cavallaro.pdf">Poster</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Cite bib" onclick="AppearBib('bib_morb')">Bibtex</a>
</div>
<div id="abstract_morb" style="display:none;" class="abstract">
  Local  image  features  play  an  important  role  in  matching  images
  under different geometric and photometric transformations.   How-
  ever,  as  the  scale  difference  across  views  increases,  the  matching
  performance may considerably decrease.   To address this problem
  we propose MORB, a multi-scale binary descriptor that is based on
  ORB and that improves the accuracy of feature matching under scale
  changes.  MORB describes an image patch at different scales using
  an  oriented  sampling  pattern  of  intensity  comparisons  in  a  prede-
  fined set of pixel pairs.  We also propose a matching strategy that
  estimates the cross-scale match between MORB descriptors across
  views.  Experiments show that MORB outperforms state-of-the-art
  binary descriptors under several transformations.
</div>
<div id="bib_morb"  style="display:none;" class="bib">@INPROCEEDINGS{Xompero2018ICIP_MORB,
  title = {{MORB: a multi-scale binary descriptor}},
  author = {Xompero, Alessio and Lanz, Oswald and Cavallaro, Andrea},
  booktitle = ICIP,
  address = {Athens, Greece},
  month = "7--10~" # oct,
  year = {2018}
}
</div>

<br>
<div>
  <div class="paper-teaser">
    <img src="images/fusion18_tds.png" alt="[img]">
  </div>
  <div class="paper-data">
    <em style="color:#008000">Multi-camera Matching of Spatio-Temporal Binary Features</em><br>
    <strong style="font-weight: bold;">A. Xompero</strong>, O. Lanz, and A. Cavallaro<br>
    International Conference on Information Fusion, Cambridge (United Kingdom), July, 10-13, 2018. <br>
    <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_fusion')">Abstract</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Download" href="projects/fusion2018/2018_FUSION_MultiCameraMatchingOfSpatioTemporalBinaryFeatures_Xompero_Lanz_Cavallaro.pdf">PDF</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Download" href="http://cis.eecs.qmul.ac.uk/2018SummerSchool/AlessioXompero_CIS_SummerSchool2018.pdf">Slides</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Cite bib" onclick="AppearBib('bib_fusion')">Bibtex</a>
  </div>
  <div id="bib_fusion"  style="display:none;" class="bib">
    @INPROCEEDINGS{Xompero2018FUSION,
    title = {{Multi-camera Matching of Spatio-Temporal Binary Features}},
    author = {Xompero, Alessio and Lanz, Oswald and Cavallaro, Andrea},
    booktitle = FUSION,
    address = {Cambridge, UK},
    month = "10--13~" # jul,
    year = {2018}
  }
</div>
<p id="abstract_fusion"  style="display:none;" class="abstract">
	Local  image  features  are  generally  robust  to  different  geometric and  photometric  transformations  on  planar surfaces or under narrow  
	baseline  views.  However,  the  matching  performance  decreases considerably  across  cameras  with unknown  poses  separated  by  a
	wide  baseline.  To  address  this problem, we accumulate temporal information within each view by  tracking  local  binary  features,  
	which  encode  intensity  comparisons  of  pixel  pairs  in  an  image patch.  We then  encode  the spatio-temporal  features  into  fixed-length
	binary  descriptors  by selecting  temporally  dominant  binary  values. We complement the descriptor with a binary vector that identifies intensity
	comparisons  that  are  temporally  unstable.  Finally, we use this additional vector to ignore the corresponding binary values in the
	fixed-length binary descriptor when matching the features across cameras. We analyse the performance of the proposed approachand  compare it with baselines. 
</p>
</div>
<br>
<div>
  <div class="paper-teaser">
    <img src="images/icassp17.png" alt="[img]">
  </div>
  <div class="paper-data">
    <em style="color:#008000">3D Mouth Tracking from a Compact Microphone Array Co-located with a Camera</em><br>
    X. Qian, <strong style="font-weight: bold;">A. Xompero</strong>, A. Brutti, O. Lanz, M. Omologo, and A. Cavallaro<br>
    International Conference on Acoustics, Speech and Signal Processing (ICASSP), Calgary (Canada), April, 15-20, 2018<br>
    <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_qianicassp')">Abstract</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Download" href="http://www.eecs.qmul.ac.uk/~andrea/papers/2018_ICASSP_3DMouthTracking_Qian_Xompero_Brutti_Lanz_Omologo_Cavallaro.pdf">PDF</a>
    &nbsp | &nbsp
    <a class="papercite_pdf" title="Cite" onclick="AppearBib('bib_qianicassp')">Bibtex</a>
  </div>
  <div id="bib_qianicassp"  style="display:none;" class="bib">
    @INPROCEEDINGS{Qian2018ICASSP,
    title = {{3D Mouth Tracking from a Compact Microphone Array Co-located with a Camera}},
    author = {Qian, Xinyuan and Xompero, Alessio and Brutti, Alessio and Lanz, Oswald and Omologo, Maurizio and Cavallaro, Andrea},
    booktitle = ICASSP,
    address = {Calgary, Canada},
    month = "15--20~" # apr,
    year = {2018}
  }
</div>
<div id="abstract_qianicassp"  style="display:none;" class="abstract">
  We address the 3D audio-visual mouth tracking problem when using a compact platform with co-located audio-visual sensors, without a
  depth camera.  In particular, we propose a multi-modal particle filter that combines a face detector and 3D hypothesis mapping to the
  image plane. The audio likelihood computation is assisted by video, which relies on a GCC-PHAT based acoustic map. By combining
  audio and video inputs, the proposed approach can cope with a reverberant and noisy environment, and can deal with situations when
  the person is occluded, outside the Field of View (FoV), or not facing the sensors.  Experimental results show that the proposed tracker is
  accurate both in 3D and on the image plane. 
</div>
<hr />
<h2 style="text-align: left;">2016</h2>
<h3>Journals</h3>
<div class="paper-teaser">
  <img src="images/cviu2016.jpg" alt="[img]">
</div>
<div class="paper-data">
  <em style="color:#008000">An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes</em><br>
  S. Ba, <span class="papercite_highlight">X. Alameda-Pineda</span>, <strong style="font-weight: bold;">A. Xompero</strong>, and R. Horaud<br>
  Computer Vision and Image Understanding, 2016<br>
  <a class="papercite_pdf" title="Cite" onclick="AppearAbstract('abstract_bacviu')">Abstract</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Download" href="http://xavirema.eu/wp-content/papercite-data/pdf/Ba-CVIU-2016.pdf">PDF</a>
  &nbsp | &nbsp
  <a class="papercite_pdf" title="Cite" onclick="AppearBib('bib_bacviu')">Bibtex</a>
</div>
<div id="bib_bacviu"  style="display:none;" class="bib">
  @ARTICLES{bA2016cviu,
  title = {An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes},
  author = {Ba, S. and Alameda-Pineda, X. and Xompero, A. and Horaud, R.},
  journal = {Computer Vision and Image Understanding},
  vol={153},
  num={},
  pages={64--76},
  month = apr,
  year = {2016},
}
</div>
<div id="abstract_bacviu"  style="display:none;" class="abstract">
 Object tracking is an ubiquitous problem that appears in many applications
 such as remote sensing, audio processing, computer vision, human-machine
 interfaces, human-robot interaction, etc.  Although thoroughly investigated
 in  computer  vision,  tracking  a  time-varying  number  of  persons  remains  a
 challenging open problem.  In this paper, we propose an on-line variational
 Bayesian model for multi-person tracking from cluttered visual observations
 provided  by  person  detectors.   The  paper  has  the  following  contributions.
 We propose a variational Bayesian framework for tracking an unknown and
 varying number of persons.  Our model results in a variational expectation-maximization  (VEM)
 algorithm  with  closed-form  expressions  both  for  the
 posterior distributions of the latent variables and for the estimation of the
 model parameters.  The proposed model exploits observations from multiple
 detectors,  and  it  is  therefore  multimodal  by  nature.   Finally,  we  propose
 to  embed  both  object-birth  and  object-visibility  processes  in  an  effort  to
 robustly  handle  temporal  appearances  and  disappearances.   Evaluated  on
 classical  multiple  person  tracking  datasets,  our  method  shows  competitive
 results with respect to state-of-the-art multiple-object tracking algorithms, 
 such as the probability hypothesis density (PHD) filter, among others.
</div>
<hr />
<h2 style="text-align: left;">2015</h2>
<h3>Master Thesis</h3>
<div>
  <em style="color:#008000">ViProT: A visual probabilistic model for moving interest points clusters tracking</em> 
  <a class="papercite_pdf" title="Download" href="projects/MasterThesis_ViProT.pdf">PDF</a><br>
  Thesis Advisors: N. Conci (UNITN), R. Horaud (INRIA Rhône-Alpes)<br>
  Co-advisors: <a href="http://xavirema.eu/">X. Alameda-Pineda</a>, S. Ba (INRIA Rhône-Alpes)<br>
</div>
</div>

<div>
  <hr />

  I also collaborated for the data collection and annotation of the following works (acknowledgments): <br><br>
  <em style="color:#008000">Detecting group formations using iBeacon technology</em> 
  <a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/14410/Tokarchuk%20Detecting%20Group%20Formations%202016%20Accepted.pdf?sequence=1">PDF</a><br>
  Kleomenis K., Hamed H., Laurissa T., and Richard G. C.<br>
  Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct (UbiComp '16)<br>
  <br>
  <em style="color:#008000">Synchronization of Multi-User Event Media at MediaEval 2015: Task Description, Datasets, and Evaluation</em> 
  <a href="https://www.semanticscholar.org/paper/Synchronization-of-Multi-User-Event-Media-at-Conci-Mezaris/3398aaea553389930485b0812570ed8ce2722e62/pdf">PDF</a><br>
  Conci, N., De Natale, F.G. and Mezaris, V.<br>
  MediaEval 2015 Workshop, Wurzen, Germany, September, 2015<br>
  <br>
  <em style="color:#008000">Synchronization of Multi-User Event Media (SEM) at MediaEval 2014: Task Description, Datasets, and Evaluation</em> 
  <a href="https://www.semanticscholar.org/paper/Synchronization-of-Multi-User-Event-Media-SEM-at-Conci-Natale/9c1929db34b53304baf48278a8a99c70c12b495c/pdf">PDF</a>
  <br>
  Conci, N., De Natale, F., Mezaris, V. and Matton, M.<br>
  MediaEval 2014 October, 2014<br>
</div>
</div>
</body>
</html>