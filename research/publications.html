<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-139569814-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-139569814-1');
  </script>

  <title>Alessio Xompero, PhD</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="image" property="og:image" content="../images/AlessioXompero.JPG">
  <link rel="shortcut icon" href="../images/profile_icon_ax.JPG" />
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <link rel="stylesheet" href="../css/mystyle.css">
  <link rel="stylesheet" href="../css/all.css">
  <link rel="stylesheet" href="../css/academicons.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css">

  <script src="../js/menu.js" defer></script>
  <script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
  
  <style>
    .publications-grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
      gap: 2rem;
      margin: 2rem 0;
    }

    .publication-item {
      border: 1px solid #e0e0e0;
      border-radius: 8px;
      overflow: hidden;
      background: white;
      transition: transform 0.2s;
    }

    .publication-item:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }

    .publication-image {
      width: 100%;
      height: 200px;
      object-fit: contain;
      background: #f5f5f5;
      display: block;
    }

    .publication-content {
      padding: 1.5rem;
    }

    .publication-title {
      font-weight: bold;
      font-size: 1.1rem;
      margin: 0.5rem 0;
      color: #333;
    }

    .publication-venue {
      font-size: 0.9rem;
      color: #666;
      margin: 0.5rem 0;
      font-style: italic;
    }

    .publication-authors {
      font-size: 0.85rem;
      color: #555;
      margin: 0.5rem 0;
      line-height: 1.4;
    }

    .publication-links {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin: 1rem 0;
      font-size: 0.85rem;
    }

    .publication-links a {
      display: inline-block;
      padding: 0.3rem 0.7rem;
      background: #f0f0f0;
      color: #0066cc;
      text-decoration: none;
      border-radius: 4px;
      transition: background 0.2s;
    }

    .publication-links a:hover {
      background: #e0e0e0;
    }

    .abstract, .bib {
      display: none;
      margin-top: 1rem;
      padding: 1rem;
      background: #f9f9f9;
      border-left: 4px solid #0066cc;
      font-size: 0.9rem;
      line-height: 1.6;
    }

    .abstract.visible, .bib.visible {
      display: block;
    }

    .bib {
      font-family: 'Courier New', monospace;
      font-size: 0.8rem;
      white-space: pre-wrap;
      word-break: break-word;
      border-left-color: #666;
    }

    .filter-section {
      margin: 2rem 0;
      padding: 1.5rem;
      background: #f5f5f5;
      border-radius: 8px;
    }

    .filter-buttons {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin-top: 1rem;
    }

    .filter-btn {
      padding: 0.5rem 1rem;
      border: 1px solid #ddd;
      background: white;
      cursor: pointer;
      border-radius: 4px;
      transition: all 0.2s;
    }

    .filter-btn:hover {
      background: #f0f0f0;
    }

    .filter-btn.active {
      background: #0066cc;
      color: white;
      border-color: #0066cc;
    }

    .intro-section {
      background: #f9f9f9;
      padding: 1.5rem;
      border-left: 4px solid #0066cc;
      margin-bottom: 2rem;
      border-radius: 4px;
      line-height: 1.6;
    }

    .altmetric-embed {
      margin-top: 0.5rem;
    }

    .publication-badge {
      margin-top: 1rem;
    }

    .thesis-info {
      margin-top: 1rem;
      padding: 0.5rem;
      background: #f9f9f9;
      font-size: 0.9rem;
    }
  </style>
</head>

<body>
  <div id="menu">
    <div class="logo">
      <a href="../index.html" class="active"><b>Alessio Xompero, Ph.D.</b></a>
    </div>
    <nav>
      <div class="menu-toggle"><span></span></div>
      <ul>      
        <li class="subnav">
          <a>Research</a>
          <div class="subnav-content">
            <a href="../research/publications.html">Publications</a>
            <a href="../research/data.html">Data</a>
            <a href="../research/software.html">Software</a>
            <a href="../research/projects.html">Projects</a>
            <a href="../research/talks.html">Talks</a>
          </div>
        </li>
        <li class="subnav">
          <a>Services</a>
          <div class="subnav-content">
            <a href="../services/reviewing.html">Reviewing</a>
            <a href="../services/openscience.html">Community</a>
          </div>
        </li>
        <li class="subnav">
          <a>Teaching</a>
          <div class="subnav-content">
            <a href="../teaching/teaching.html">Teaching</a>
          </div>
        </li>
        <li class="subnav">
          <a>News</a>
          <div class="subnav-content">
            <a href="../news.html">All news</a>
            <a href="../blog.html">Blog</a>
          </div>
        </li>
        <li class="subnav">
          <a>About</a>
          <div class="subnav-content">
            <a href="../bio.html">Biography</a>
            <a href="../gallery.html">Gallery</a>
          </div>
        </li>
      </ul>
    </nav>
  </div>

  <div id="section">
    <div class="scrolling-content">
      <h1>Publications</h1>
      
      <article class="intro-section">
        <p>This page presents my research publications covering topics in computer vision, audio-visual signal processing, and robotics. My research interests include multi-view matching, feature descriptors, object tracking, human-robot interaction, visual privacy, and robotic affordances. The publications are organized by category and can be filtered by type and year.</p>
      </article>

      <div class="filter-section">
        <h3>Filter by Category:</h3>
        <div class="filter-buttons">
          <button class="filter-btn active" data-filter="all">All Publications</button>
          <button class="filter-btn" data-filter="chapters">Book Chapters</button>
          <button class="filter-btn" data-filter="journals">Journals</button>
          <button class="filter-btn" data-filter="conferences">Conferences</button>
          <button class="filter-btn" data-filter="workshops">Workshops</button>
          <button class="filter-btn" data-filter="preprints">Pre-prints</button>
          <button class="filter-btn" data-filter="thesis">Thesis</button>
        </div>
      </div>

      <div class="filter-section">
        <h3>Filter by Year:</h3>
        <div class="filter-buttons" id="year-filters">
          <!-- Year filters will be populated by JavaScript -->
        </div>
      </div>

      <section class="publications-grid" id="publications-container">
        <!-- BOOK CHAPTERS -->
        <article class="publication-item" data-filter="chapters" data-year="2022">
          <img src="../images/visualadversarial.png" alt="Visual adversarial attacks and defenses" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Chapter 15 - Visual adversarial attacks and defenses</div>
            <div class="publication-venue">In Advanced Methods And Deep Learning In Computer Vision, Editors: E. R. Davies, Matthew Turk, 1st Edition, Elsevier, pages 511-543, November 2022</div>
            <div class="publication-authors">C. Oh, <b>A. Xompero</b>, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_chapter22')">abstract</a>
              <a href="https://doi.org/10.1016/B978-0-12-822109-9.00024-2" target="_blank">chapter</a>
              <a href="#" onclick="toggleBib(event, 'bib_chapter22')">bibtex</a>
              <a href="https://cis.eecs.qmul.ac.uk/visualadversarial.html" target="_blank">webpage</a>
            </div>
            <div id="abstract_chapter22" class="abstract">Visual adversarial examples are images and videos purposefully perturbed to mislead machine learning models. This chapter presents an overview of methods that craft adversarial perturbations to generate visual adversarial examples for image classification, object detection, motion estimation and video recognition tasks. We define the key properties of an adversarial attack and the types of perturbations that an attack generates. We then analyze the main design choices for methods that craft adversarial attacks for images and videos, and discuss the knowledge they use of the target model. Finally, we review defense mechanisms that increase the robustness of machine learning models to adversarial attacks or detect manipulated input data.</div>
            <div id="bib_chapter22" class="bib">@incollection{Oh2022VisualAdversarial,
  title = {Chapter 15 - Visual adversarial attacks and defenses},
  author = {Changjae Oh and Alessio Xompero and Andrea Cavallaro},
  booktitle = {Advanced Methods and Deep Learning in Computer Vision},
  editor = {E.R. Davies and Matthew A. Turk},
  series = {Computer Vision and Pattern Recognition},
  publisher = {Academic Press},
  pages = {511-543},
  year = {2022},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1016/B978-0-12-822109-9.00024-2" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <!-- JOURNALS -->
        <article class="publication-item" data-filter="journals" data-year="2025">
          <img src="../images/popets25_v2.png" alt="Learning Privacy from Visual Entities" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Learning Privacy from Visual Entities</div>
            <div class="publication-venue">Proceedings on Privacy Enhancing Technologies (PoPETs), vol. 2025, n. 3, pp. 1-21, March 2025</div>
            <div class="publication-authors"><b>A. Xompero</b> and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_popets25')">abstract</a>
              <a href="https://doi.org/10.56553/popets-2025-0098" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2503.12464" target="_blank">arxiv</a>
              <a href="#" onclick="toggleBib(event, 'bib_popets25')">bibtex</a>
              <a href="https://github.com/graphnex/privacy-from-visual-entities" target="_blank">code</a>
              <a href="https://doi.org/10.5281/zenodo.15348506" target="_blank">data</a>
              <a href="https://doi.org/10.5281/zenodo.15349470" target="_blank">models</a>
              <a href="https://graphnex.github.io/privacy-from-visual-entities/" target="_blank">webpage</a>
            </div>
            <div id="abstract_popets25" class="abstract">Subjective interpretation and content diversity make predicting whether an image is private or public a challenging task. Graph neural networks combined with convolutional neural networks (CNNs), which consist of 14,000 to 500 millions parameters, generate features for visual entities (e.g., scene and object types) and identify the entities that contribute to the decision. In this paper, we show that using a simpler combination of transfer learning and a CNN to relate privacy with scene types optimises only 732 parameters while achieving comparable performance to that of graph-based methods. On the contrary, end-to-end training of graph-based methods can mask the contribution of individual components to the classification performance. Furthermore, we show that a high-dimensional feature vector, extracted with CNNs for each visual entity, is unnecessary and complexifies the model. The graph component has also negligible impact on performance, which is driven by fine-tuning the CNN to optimise image features for privacy nodes.</div>
            <div id="bib_popets25" class="bib">@Article{Xompero2025PoPETs,
  title = {Learning Privacy from Visual Entities},
  author = {Xompero, A. and Cavallaro, A.},
  journal = {Proceedings on Privacy Enhancing Technologies},
  volume = {2025},
  number = {3},
  pages={1--21},
  month = {Mar},
  year = {2025},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.56553/popets-2025-0098" data-arxiv-id="10.48550/arXiv.2503.12464" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="journals" data-year="2025">
          <img src="../images/pang24stereoho.jpg" alt="Stereo Hand-Object Reconstruction" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Stereo Hand-Object Reconstruction for Human-to-Robot Handover</div>
            <div class="publication-venue">IEEE Robotics and Automation Letters, Vol.10, n.6, June 2025. To be presented at IEEE/RSJ Int. Conf. Intell. Robots and Systems (IROS), Hangzhou, China, 19-25 October 2025</div>
            <div class="publication-authors">Y. L. Pang, <b>A. Xompero</b>, C. Oh, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_pang25ral')">abstract</a>
              <a href="https://doi.org/10.1109/LRA.2025.3562790" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2412.07487" target="_blank">arxiv</a>
              <a href="#" onclick="toggleBib(event, 'bib_pang25ral')">bibtex</a>
              <a href="https://github.com/QM-IPAlab/StereoHO" target="_blank">code</a>
              <a href="https://qm-ipalab.github.io/StereoHO/" target="_blank">webpage</a>
              <a href="https://qm-ipalab.github.io/StereoHO/static/videos/iros_final_720_compress.mp4" target="_blank">video</a>
              <a href="../resources/posters/Pang2025_StereoHO_BMVA.pdf" target="_blank">poster</a>
            </div>
            <div id="abstract_pang25ral" class="abstract">Jointly estimating hand and object shape facilitates the grasping task in human-to-robot handovers. Relying on hand-crafted prior knowledge about the geometric structure of the object fails when generalising to unseen objects, and depth sensors fail to detect transparent objects such as drinking glasses. In this work, we propose a method for hand-object reconstruction that combines single-view reconstructions probabilistically to form a coherent stereo reconstruction. We learn 3D shape priors from a large synthetic hand-object dataset, and use RGB inputs to better capture transparent objects. We show that our method reduces the object Chamfer distance compared to existing RGB based hand-object reconstruction methods on single view and stereo settings. We process the reconstructed hand-object shape with a projection-based outlier removal step and use the output to guide a human-to-robot handover pipeline with wide-baseline stereo RGB cameras. Our hand-object reconstruction enables a robot to successfully receive a diverse range of household objects from the human.</div>
            <div id="bib_pang25ral" class="bib">@article{Pang2025RAL,
  title = {Stereo Hand-Object Reconstruction for Human-to-Robot Handover},
  author = {Pang, Y. L. and Xompero, A. and Oh, C. and Cavallaro, A.},
  journal={IEEE Robotics and Automation Letters},
  year={2025},
  volume={10},
  number={6},
  pages={5761--5768},
  doi={10.1109/LRA.2025.3562790}
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/LRA.2025.3562790" data-arxiv-id="10.48550/arXiv.2412.07487" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="journals" data-year="2024">
          <img src="../images/ram24_handover.png" alt="Robotic Grasping and Manipulation Competition" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Robotic Grasping and Manipulation Competition at the 2024 IEEE/RAS International Conference on Robotics and Automation</div>
            <div class="publication-venue">IEEE Robotics & Automation Magazine, Competitions, vol. 31, n. 4, pp. 174-185, December 2024</div>
            <div class="publication-authors">Y. Sun, B. Calli, K. Kimble, F. wyffels, V. De Gusseme, K. Hang, S. D'Avella, <b>A. Xompero</b>, A. Cavallaro, M. A. Roa, J. Avendano, A. Mavrommati</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_ram24')">abstract</a>
              <a href="https://doi.org/10.1109/MRA.2024.3481609" target="_blank">paper</a>
              <a href="#" onclick="toggleBib(event, 'bib_ram24')">bibtex</a>
              <a href="https://cse.usf.edu/~yusun/rgmc/2024.html" target="_blank">webpage</a>
            </div>
            <div id="abstract_ram24" class="abstract">The Ninth Robotic Grasping and Manipulation Competition (RGMC) took place in Yokohama, Japan, during the 2024 IEEE/RAS International Conference on Robotics and Automation (ICRA). The series of RGMC events started in 2016 at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) with strong support from the conference's organization committee, and since then they have been held each year at ICRA or IROS. Across the editions, RGMC engaged the community in solving the open challenges associated with various robotic grasping and manipulation tasks for manufacturing, service robots, and logistics, and in advancing research and technology towards more realistic scenarios that can be encountered in daily activities at home or in warehouses.</div>
            <div id="bib_ram24" class="bib">@Article{Sun2024RAM,
  title = {Robotic Grasping and Manipulation Competition at the 2024 IEEE/RAS International Conference on Robotics and Automation},
  author = {Sun, Y., and Calli, B. and Kimble, K. and wyffels, F. and De Gusseme, V. and Hang, K. and D'Avella, S. and Xompero, A. and Cavallaro, A. and Roa, M. A. and Avendano, J. and Mavrommati, A.},
  journal = {IEEE Robotics and Automation Magazine},
  volume = {31},
  number = {4},
  pages={174--185},
  month = {Dec},
  year = {2024},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/MRA.2024.3481609" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="journals" data-year="2022">
          <img src="../images/setup.png" alt="CORSMAL Benchmark" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">The CORSMAL Benchmark for the Prediction of the Properties of Containers</div>
            <div class="publication-venue">IEEE Access, vol. 10, 2022</div>
            <div class="publication-authors"><b>A. Xompero</b>, S. Donaher, V. Iashin, F. Palermo, G. Solak, C. Coppola, R. Ishikawa, Y. Nagao, R. Hachiuma, Q. Liu, F. Feng, C. Lan, R. H. M. Chan, G. Christmann, J. Song, G. Neeharika, C. K. T. Reddy, D. Jain, B. U. Rehman, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_access22')">abstract</a>
              <a href="https://doi.org/10.1109/ACCESS.2022.3166906" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2107.12719" target="_blank">arxiv</a>
              <a href="#" onclick="toggleBib(event, 'bib_access22')">bibtex</a>
              <a href="https://corsmal.github.io/challenge.html" target="_blank">webpage</a>
            </div>
            <div id="abstract_access22" class="abstract">The contactless estimation of the weight of a container and the amount of its content manipulated by a person are key pre-requisites for safe human-to-robot handovers. However, opaqueness and transparencies of the container and the content, and variability of materials, shapes, and sizes, make this problem challenging. In this paper, we present a range of methods and an open framework to benchmark acoustic and visual perception for the estimation of the capacity of a container, and the type, mass, and amount of its content.</div>
            <div id="bib_access22" class="bib">@Article{Xompero2022Access,
  title = {The CORSMAL Benchmark for the Prediction of the Properties of Containers},
  author = {Xompero, A. and Donaher, S. and Iashin, V. and Palermo, F. and Solak, G. and Coppola, C. and Ishikawa, R. and Nagao, Y. and Hachiuma, R. and Liu, Q. and Feng, F. and Lan, C. and Chan, R. H. M. and Christmann, G. and Song, J. and Neeharika, G. and Reddy, C. K. T. and Jain, D. and Rehman, B. U. and Cavallaro, A.},
  journal = {IEEE Access},
  volume = {10},
  pages={41388--41402},
  month = {Apr},
  year = {2022},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/ACCESS.2022.3166906" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="journals" data-year="2020">
          <img src="../images/tip20_mst.png" alt="TIP 2020 MST" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">A spatio-temporal multi-scale binary descriptor</div>
            <div class="publication-venue">IEEE Transactions on Image Processing, vol. 29, no. 1, pp. 4362-4375, Dec. 2020</div>
            <div class="publication-authors"><b>A. Xompero</b>, O. Lanz, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_xomperotip')">abstract</a>
              <a href="https://doi.org/10.1109/TIP.2020.2965277" target="_blank">paper</a>
              <a href="../resources/Xompero_Lanz_Cavallaro_TIP_2020_MST.pdf" target="_blank">preprint</a>
              <a href="#" onclick="toggleBib(event, 'bib_xomperotip')">bibtex</a>
              <a href="MST/mst.html" target="_blank">webpage</a>
            </div>
            <div id="abstract_xomperotip" class="abstract">Binary descriptors are widely used for multi-view matching and robotic navigation. However, their matching performance decreases considerably under severe scale and viewpoint changes in non-planar scenes. To overcome this problem, we propose to encode the varying appearance of selected 3D scene points tracked by a moving camera with compact spatio-temporal descriptors. To this end, we first track interest points and capture their temporal variations at multiple scales. Then, we validate feature tracks through 3D reconstruction and compress the temporal sequence of descriptors by encoding the most frequent and stable binary values. Finally, we determine multi-scale correspondences across views with a matching strategy that handles severe scale differences.</div>
            <div id="bib_xomperotip" class="bib">@Article{Xompero2020TIP_MST,
  title={A spatio-temporal multi-scale binary descriptor},
  author={Alessio Xompero and Oswald Lanz and Andrea Cavallaro},
  journal={IEEE Transactions on Image Processing},
  volume={29},
  number={1},
  pages={4362--4375},
  month={Dec},
  year={2020},
  issn={1057-7149},
  doi={10.1109/TIP.2020.2965277}
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/TIP.2020.2965277" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="journals" data-year="2020">
          <img src="http://corsmal.github.io/benchmark/resources/handover.gif" alt="RAL 2020" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Benchmark for Human-to-Robot Handovers of Unseen Containers with Unknown Filling</div>
            <div class="publication-venue">IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 1642-1649, Apr. 2020</div>
            <div class="publication-authors">R. Sanchez-Matilla, K. Chatzilygeroudis, A. Modas, N. Ferreira Duarte, <b>A. Xompero</b>, P. Frossard, A. Billard, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_sanchezral')">abstract</a>
              <a href="https://doi.org/10.1109/LRA.2020.2969200" target="_blank">paper</a>
              <a href="http://corsmal.github.io/benchmark.html" target="_blank">webpage</a>
              <a href="#" onclick="toggleBib(event, 'bib_sanchezral')">bibtex</a>
              <a href="https://github.com/CORSMAL/Benchmark" target="_blank">code</a>
              <a href="http://corsmal.github.io/benchmark/resources/Benchmark.mp4" target="_blank">video</a>
            </div>
            <div id="abstract_sanchezral" class="abstract">The real-time estimation through vision of the physical properties of objects manipulated by humans is important to inform the control of robots for performing accurate and safe grasps of objects handed over by humans. However, estimating the 3D pose and dimensions of previously unseen objects using only RGB cameras is challenging due to illumination variations, reflective surfaces, transparencies, and occlusions caused both by the human and the robot. In this paper we present a benchmark for dynamic human-to-robot handovers that do not rely on a motion capture system, markers, or prior knowledge of specific objects.</div>
            <div id="bib_sanchezral" class="bib">@Article{SanchezMatilla2020RAL_Benchmark,
  title={Benchmark for Human-to-Robot Handovers of Unseen Containers with Unknown Filling},
  author={Ricardo Sanchez-Matilla, Konstantinos Chatzilygeroudis, Apostolos Modas, Nuno Ferreira Duarte, Alessio Xompero, Pascal Frossard, Aude Billard, and Andrea Cavallaro},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={2},
  pages={1642--1649},
  month={Apr},
  year={2020},
  issn={2377-3766},
  doi={10.1109/LRA.2020.2969200}
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/LRA.2020.2969200" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="journals" data-year="2016">
          <img src="../images/cviu2016.jpg" alt="CVIU 2016" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes</div>
            <div class="publication-venue">Computer Vision and Image Understanding, 2016</div>
            <div class="publication-authors">S. Ba, X. Alameda-Pineda, <b>A. Xompero</b>, and R. Horaud</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_bacviu')">abstract</a>
              <a href="http://xavirema.eu/wp-content/papercite-data/pdf/Ba-CVIU-2016.pdf" target="_blank">preprint</a>
              <a href="#" onclick="toggleBib(event, 'bib_bacviu')">bibtex</a>
              <a href="https://youtu.be/ZtIM3Ikpb3k" target="_blank">video</a>
            </div>
            <div id="abstract_bacviu" class="abstract">Object tracking is an ubiquitous problem that appears in many applications such as remote sensing, audio processing, computer vision, human-machine interfaces, human-robot interaction, etc. Although thoroughly investigated in computer vision, tracking a time-varying number of persons remains a challenging open problem. In this paper, we propose an on-line variational Bayesian model for multi-person tracking from cluttered visual observations provided by person detectors.</div>
            <div id="bib_bacviu" class="bib">@article{Ba2016cviu,
  title = {An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes},
  author = {Ba, S. and Alameda-Pineda, X. and Xompero, A. and Horaud, R.},
  journal = {Computer Vision and Image Understanding},
  volume={153},
  pages={64--76},
  month = apr,
  year = {2016},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1016/j.cviu.2016.07.006" data-arxiv-id="10.48550/arXiv.1509.01520" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <!-- CONFERENCES -->
        <article class="publication-item" data-filter="conferences" data-year="2022">
          <img src="../images/overviewpaper_main.png" alt="ICASSP 2022" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Audio-Visual Object Classification for Human-Robot Collaboration</div>
            <div class="publication-venue">IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP), Singapore and Virtual, 22-27 May 2022</div>
            <div class="publication-authors"><b>A. Xompero</b>, Y. L. Pang, T. Patten, A. Prabhakar, B. Calli, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_icassp22')">abstract</a>
              <a href="https://doi.org/10.1109/ICASSP43922.2022.9746336" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2203.01977" target="_blank">arxiv</a>
              <a href="https://corsmal.github.io/challenge.html" target="_blank">challenge</a>
              <a href="#" onclick="toggleBib(event, 'bib_icassp22')">bibtex</a>
            </div>
            <div id="abstract_icassp22" class="abstract">Human-robot collaboration requires the contactless estimation of the physical properties of containers manipulated by a person, for example while pouring content in a cup or moving a food box. Acoustic and visual signals can be used to estimate the physical properties of such objects, which may vary substantially in shape, material and size, and also be occluded by the hands of the person. To facilitate comparisons and stimulate progress in solving this problem, we present the CORSMAL challenge and a dataset to assess the performance of the algorithms through a set of well-defined performance scores.</div>
            <div id="bib_icassp22" class="bib">@InProceedings{Xompero2022ICASSP,
  title = {Audio-Visual Object Classification for Human-Robot Collaboration},
  author = {Xompero, A. and Pang, Y. L. and Patten, T. and Prabhakar, A. and Calli, B. and Cavallaro, A.},
  booktitle = {IEEE International Conference on Acoustic, Speech and Signal Processing},
  address={Singapore},
  month="22--27~" # MAY,
  year = {2022},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/ICASSP43922.2022.9746336" data-arxiv-id="10.48550/arXiv.2203.01977" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="conferences" data-year="2021">
          <img src="http://corsmal.github.io/images/real2sim_setup.png" alt="RO-MAN 2021" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Towards safe human-to-robot handovers of unknown containers</div>
            <div class="publication-venue">IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), Virtual, 8-12 Aug 2021</div>
            <div class="publication-authors">Y. L. Pang, <b>A. Xompero</b>, C. Oh, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_roman21')">abstract</a>
              <a href="https://doi.org/10.1109/RO-MAN50785.2021.9515350" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2107.01309" target="_blank">arxiv</a>
              <a href="http://corsmal.github.io/safe_handover.html" target="_blank">webpage</a>
              <a href="#" onclick="toggleBib(event, 'bib_roman21')">bibtex</a>
            </div>
            <div id="abstract_roman21" class="abstract">Safe human-to-robot handovers of unknown containers require accurate estimation of the human and object properties, such as hand pose, object shape, trajectory, and weight. However, accurate estimation requires the use of expensive equipment, such as motion capture systems, markers, and 3D object models. Moreover, developing and testing on real hardware can be dangerous for the human and object. We propose a real-to-simulation framework to conduct safe human-to-robot handovers with visual estimations of the physical properties of unknown cups or drinking glasses and of the human hands from videos of a person manipulating the object.</div>
            <div id="bib_roman21" class="bib">@Article{Pang2021ROMAN,
  title = {Towards safe human-to-robot handovers of unknown containers},
  author = {Yik Lang Pang and Alessio Xompero and Changjae Oh and Andrea Cavallaro},
  booktitle = {IEEE International Conference on Robot and Human Interactive Communication},
  address = {Virtual},
  month = "8-12~" # aug,
  year = {2021},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/RO-MAN50785.2021.9515350" data-arxiv-id="10.48550/arXiv.2107.01309" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="conferences" data-year="2021">
          <img src="http://corsmal.github.io/images/RTL/abstract.png" alt="ICIP 2021" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Improving filling level classification with adversarial training</div>
            <div class="publication-venue">IEEE International Conference on Image Processing (ICIP), Anchorage, Alaska, USA, 19-22 Sep 2021</div>
            <div class="publication-authors">A. Modas, <b>A. Xompero</b>, R. Sanchez-Matilla, P. Frossard, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_icip21')">abstract</a>
              <a href="https://doi.org/10.1109/ICIP42928.2021.9506112" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2102.04057" target="_blank">arxiv</a>
              <a href="http://corsmal.github.io/filling.html" target="_blank">webpage</a>
              <a href="#" onclick="toggleBib(event, 'bib_icip21')">bibtex</a>
              <a href="https://zenodo.org/record/4642577" target="_blank">data</a>
              <a href="https://zenodo.org/record/4518951" target="_blank">models</a>
            </div>
            <div id="abstract_icip21" class="abstract">We investigate the problem of classifying -- from a single image -- the level of content in a cup or a drinking glass. This problem is made challenging by several ambiguities caused by transparencies, shape variations and partial occlusions, and by the availability of only small training datasets. In this paper, we tackle this problem with an appropriate strategy for transfer learning. Specifically, we use adversarial training in a generic source dataset and then refine the training with a task-specific dataset.</div>
            <div id="bib_icip21" class="bib">@Article{Modas2021ICIP,
  title = {Improving filling level classification with adversarial training},
  author = {Apostolos Modas and Alessio Xompero and Ricardo Sanchez-Matilla and Pascal Frossard and Andrea Cavallaro},
  booktitle = {IEEE International Conference on Image Processing},
  address = {Anchorage, Alaska, USA},
  month = "19-22~" # sep,
  year = {2021},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/ICIP42928.2021.9506112" data-arxiv-id="10.48550/arXiv.2102.04057" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="conferences" data-year="2021">
          <img src="http://corsmal.github.io/images/acc_image.png" alt="EUSIPCO 2021" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Audio classification of the content of food containers and drinking glasses</div>
            <div class="publication-venue">European Signal Processing Conference (EUSIPCO), Dublin, Ireland, 23-27 August 2021</div>
            <div class="publication-authors">S. Donaher, <b>A. Xompero</b>, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_eusipco21')">abstract</a>
              <a href="https://doi.org/10.23919/EUSIPCO54536.2021.9616206" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2103.15999" target="_blank">arxiv</a>
              <a href="http://corsmal.github.io/audio_classification.html" target="_blank">webpage</a>
              <a href="#" onclick="toggleBib(event, 'bib_eusipco21')">bibtex</a>
              <a href="https://github.com/CORSMAL/ACC/" target="_blank">code</a>
              <a href="https://corsmal.github.io/containers_manip.html" target="_blank">data</a>
            </div>
            <div id="abstract_eusipco21" class="abstract">Food containers, drinking glasses and cups handled by a person generate sounds that vary with the type and amount of their content. In this paper, we propose a new model for sound-based classification of the type and amount of content in a container. The proposed model is based on the decomposition of the problem into two steps, namely action recognition and content classification. We consider the scenario of the recent CORSMAL Containers Manipulation dataset and consider two actions (shaking and pouring), and seven material and filling level combinations.</div>
            <div id="bib_eusipco21" class="bib">@Article{Donhaer2021EUSIPCO,
  title={Audio classification of the content of food containers and drinking glasses},
  author={Santiago Donhaer and Alessio Xompero and Andrea Cavallaro},
  booktitle={European Signal Processing Conference},
  address={Dublin, Ireland},
  month = "23-27~" # aug,
  year={2021},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.23919/EUSIPCO54536.2021.9616206" data-arxiv-id="10.48550/arXiv.2103.15999" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="conferences" data-year="2020">
          <img src="../images/diagram_ICASSP20.png" alt="ICASSP 2020" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Multi-view shape estimation of transparent containers</div>
            <div class="publication-venue">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 4-8 May 2020</div>
            <div class="publication-authors"><b>A. Xompero</b>, R. Sanchez-Matilla, A. Modas, P. Frossard, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_xomperoicassp')">abstract</a>
              <a href="https://doi.org/10.1109/ICASSP40776.2020.9054112" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.1911.12354" target="_blank">arxiv</a>
              <a href="http://corsmal.github.io/LoDE.html" target="_blank">webpage</a>
              <a href="#" onclick="toggleBib(event, 'bib_xomperoicassp')">bibtex</a>
              <a href="http://corsmal.github.io/containers.html" target="_blank">data</a>
              <a href="https://github.com/CORSMAL/LoDE" target="_blank">code</a>
              <a href="https://www.youtube.com/watch?v=3UquyP2QXI4" target="_blank">video</a>
            </div>
            <div id="abstract_xomperoicassp" class="abstract">The 3D localisation of an object and the estimation of its properties, such as shape and dimensions, are challenging under varying degrees of transparency and lighting conditions. In this paper, we propose a method for jointly localising container-like objects and estimating their dimensions using two wide-baseline, calibrated RGB cameras. Under the assumption of circular symmetry along the vertical axis, we estimate the dimensions of an object with a generative 3D sampling model of sparse circumferences, iterative shape fitting and image re-projection to verify the sampling hypotheses in each camera using semantic segmentation masks.</div>
            <div id="bib_xomperoicassp" class="bib">@InProceedings{Xompero2020ICASSP_LoDE,
  title={Multi-view shape estimation of transparent containers},
  author={Alessio Xompero and Ricardo Sanchez-Matilla and Apostolos Modas and Pascal Frossard and Andrea Cavallaro},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},
  address = {Barcelona, Spain},
  month = "4-8~" # may,
  year = {2020}
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/ICASSP40776.2020.9054112" data-arxiv-id="10.48550/arXiv.1911.12354" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="conferences" data-year="2019">
          <img src="../images/multisensor.png" alt="ICASSP 2019" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Accurate Target Annotation in 3D from Multimodal Streams</div>
            <div class="publication-venue">International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton (UK), May, 12-17, 2019</div>
            <div class="publication-authors">O. Lanz, A. Brutti, <b>A. Xompero</b>, X. Qian, M. Omologo, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_lanzicassp')">abstract</a>
              <a href="https://doi.org/10.1109/ICASSP.2019.8682619" target="_blank">paper</a>
              <a href="#" onclick="toggleBib(event, 'bib_lanzicassp')">bibtex</a>
              <a href="https://speechtek.fbk.eu/cav3d-dataset/" target="_blank">CAV3D data</a>
            </div>
            <div id="abstract_lanzicassp" class="abstract">Accurate annotation is fundamental to quantify the performance of multi-sensor and multi-modal object detectors and trackers. However, invasive or expensive instrumentation is needed to automatically generate these annotations. To mitigate this problem, we present a multi-modal approach that leverages annotations from reference streams (e.g. individual camera views) and measurements from unannotated additional streams (e.g. audio) to infer 3D trajectories through an optimization. The core of our approach is a multi-modal extension of Bundle Adjustment with a cross-modal correspondence detection that selectively uses measurements in the optimization.</div>
            <div id="bib_lanzicassp" class="bib">@INPROCEEDINGS{Lanz2019ICASSP,
  title = {Accurate target annotation in 3D from multimodal streams},
  author = {Lanz, Oswald and Brutti, Alessio and Xompero, Alessio and Qian, Xinyuan and Omologo, Maurizio and Cavallaro, Andrea},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},
  address = {Brighton, UK},
  month = "12--17~" # may,
  year = {2019}
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/ICASSP.2019.8682619" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="conferences" data-year="2018">
          <img src="../images/MORB.png" alt="ICIP 2018" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">MORB: a multi-scale binary descriptor</div>
            <div class="publication-venue">IEEE International Conference on Image Processing (ICIP), Athens (Greece), October, 7-10, 2018</div>
            <div class="publication-authors"><b>A. Xompero</b>, O. Lanz, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_morb')">abstract</a>
              <a href="https://doi.org/10.1109/ICIP.2018.8451024" target="_blank">paper</a>
              <a href="#" onclick="toggleBib(event, 'bib_morb')">bibtex</a>
              <a href="projects/icip2018/2018_ICIP_poster_Xompero_Lanz_Cavallaro.pdf" target="_blank">poster</a>
              <a href="https://github.com/kerolex/MORB" target="_blank">code</a>
            </div>
            <div id="abstract_morb" class="abstract">Local image features play an important role in matching images under different geometric and photometric transformations. However, as the scale difference across views increases, the matching performance may considerably decrease. To address this problem we propose MORB, a multi-scale binary descriptor that is based on ORB and that improves the accuracy of feature matching under scale changes. MORB describes an image patch at different scales using an oriented sampling pattern of intensity comparisons in a predefined set of pixel pairs.</div>
            <div id="bib_morb" class="bib">@INPROCEEDINGS{Xompero2018ICIP_MORB,
  title = {{MORB: a multi-scale binary descriptor}},
  author = {Xompero, Alessio and Lanz, Oswald and Cavallaro, Andrea},
  booktitle = {IEEE International Conference on Image Processing},
  address = {Athens, Greece},
  month = "7--10~" # oct,
  year = {2018}
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/ICIP.2018.8451024" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="conferences" data-year="2018">
          <img src="../images/fusion18_tds.png" alt="FUSION 2018" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Multi-camera Matching of Spatio-Temporal Binary Features</div>
            <div class="publication-venue">International Conference on Information Fusion (FUSION), Cambridge (United Kingdom), July, 10-13, 2018</div>
            <div class="publication-authors"><b>A. Xompero</b>, O. Lanz, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_fusion')">abstract</a>
              <a href="https://doi.org/10.23919/ICIF.2018.8455444" target="_blank">paper</a>
              <a href="#" onclick="toggleBib(event, 'bib_fusion')">bibtex</a>
              <a href="http://cis.eecs.qmul.ac.uk/2018SummerSchool/AlessioXompero_CIS_SummerSchool2018.pdf" target="_blank">slides</a>
            </div>
            <div id="abstract_fusion" class="abstract">Local image features are generally robust to different geometric and photometric transformations on planar surfaces or under narrow baseline views. However, the matching performance decreases considerably across cameras with unknown poses separated by a wide baseline. To address this problem, we accumulate temporal information within each view by tracking local binary features, which encode intensity comparisons of pixel pairs in an image patch.</div>
            <div id="bib_fusion" class="bib">@INPROCEEDINGS{Xompero2018FUSION,
  title = {{Multi-camera Matching of Spatio-Temporal Binary Features}},
  author = {Xompero, Alessio and Lanz, Oswald and Cavallaro, Andrea},
  booktitle = {International Conference on Information Fusion},
  address = {Cambridge, UK},
  month = "10--13~" # jul,
  year = {2018}
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.23919/ICIF.2018.8455444" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="conferences" data-year="2018">
          <img src="../images/icassp17.png" alt="ICASSP 2018" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">3D Mouth Tracking from a Compact Microphone Array Co-located with a Camera</div>
            <div class="publication-venue">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Calgary (Canada), April, 15-20, 2018</div>
            <div class="publication-authors">X. Qian, <b>A. Xompero</b>, A. Brutti, O. Lanz, M. Omologo, and A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_qianicassp')">abstract</a>
              <a href="https://doi.org/10.1109/ICASSP.2018.8461323" target="_blank">paper</a>
              <a href="#" onclick="toggleBib(event, 'bib_qianicassp')">bibtex</a>
            </div>
            <div id="abstract_qianicassp" class="abstract">We address the 3D audio-visual mouth tracking problem when using a compact platform with co-located audio-visual sensors, without a depth camera. In particular, we propose a multi-modal particle filter that combines a face detector and 3D hypothesis mapping to the image plane. The audio likelihood computation is assisted by video, which relies on a GCC-PHAT based acoustic map. By combining audio and video inputs, the proposed approach can cope with a reverberant and noisy environment, and can deal with situations when the person is occluded, outside the Field of View (FoV), or not facing the sensors.</div>
            <div id="bib_qianicassp" class="bib">@INPROCEEDINGS{Qian2018ICASSP,
  title = {{3D Mouth Tracking from a Compact Microphone Array Co-located with a Camera}},
  author = {Qian, Xinyuan and Xompero, Alessio and Brutti, Alessio and Lanz, Oswald and Omologo, Maurizio and Cavallaro, Andrea},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},
  address = {Calgary, Canada},
  month = "15--20~" # apr,
  year = {2018}
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/ICASSP.2018.8461323" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <!-- WORKSHOPS -->
        <article class="publication-item" data-filter="workshops" data-year="2024">
          <img src="../images/acvr2024.png" alt="ACVR 2024" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Segmenting Object Affordances: Reproducibility and Sensitivity to Scale</div>
            <div class="publication-venue">Twelth International Workshop on Assistive Computer Vision and Robotics (ACVR), European Conference on Computer Vision (ECCV), Milan (Italy), 29 September 2024</div>
            <div class="publication-authors">T. Apicella, <b>A. Xompero</b>, P. Gastaldo, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_acvr2024')">abstract</a>
              <a href="https://doi.org/10.48550/arXiv.2409.01814" target="_blank">arxiv</a>
              <a href="#" onclick="toggleBib(event, 'bib_acvr2024')">bibtex</a>
              <a href="https://apicis.github.io/aff-seg" target="_blank">webpage</a>
              <a href="https://apicis.github.io/aff-seg/assets/Apicella2024ECCVw_ACVR_Poster.png" target="_blank">poster</a>
              <a href="https://github.com/apicis/aff-seg" target="_blank">code</a>
            </div>
            <div id="abstract_acvr2024" class="abstract">Visual affordance segmentation identifies image regions of an object an agent can interact with. Existing methods re-use and adapt learning-based architectures for semantic segmentation to the affordance segmentation task and evaluate on small-size datasets. However, experimental setups are often not reproducible, thus leading to unfair and inconsistent comparisons. In this work, we benchmark these methods under a reproducible setup on two single objects scenarios, tabletop without occlusions and hand-held containers, to facilitate future comparisons.</div>
            <div id="bib_acvr2024" class="bib">@InProceedings{Apicella2024ACVR_ECCVW,
  title = {Segmenting Object Affordances: Reproducibility and Sensitivity to Scale},
  author = {Apicella, T. and Xompero, A. and Gastaldo, P. and Cavallaro, A.},
  booktitle = {Proceedings of the European Conference on Computer Vision Workshops},
  note = {Twelfth International Workshop on Assistive Computer Vision and Robotics},
  address={Milan, Italy},
  month="29" # SEP,
  year = {2024},
}</div>
           <div class="publication-badge">
              <div data-badge-type="medium-donut" data-arxiv-id="10.48550/arXiv.2409.01814" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="workshops" data-year="2024">
          <img src="../images/ig-privacy.png" alt="XAI4CV 2024" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Explaining models relating objects and privacy</div>
            <div class="publication-venue">The 3rd Explainable AI for Computer Vision (XAI4CV) Workshop, IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR), Seattle (US), 18 June 2024</div>
            <div class="publication-authors"><b>A. Xompero</b>, M. Bontonou, J. Arbona, E. Benetos, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_cvpr2024')">abstract</a>
              <a href="https://openaccess.thecvf.com/content/CVPR2024W/XAI4CV/papers/Xompero_Explaining_Models_Relating_Objects_and_Privacy_CVPRW_2024_paper.pdf" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2405.01646" target="_blank">arxiv</a>
              <a href="#" onclick="toggleBib(event, 'bib_cvpr2024')">bibtex</a>
              <a href="resources/Xompero2024CVPRw_XAI4CV_Poster.pdf" target="_blank">poster</a>
              <a href="https://youtu.be/mCckSgctoCY?si=MP4GQBM_lohCwzQS" target="_blank">video</a>
              <a href="https://github.com/graphnex/ig-privacy" target="_blank">code</a>
              <a href="https://graphnex.github.io/ig-privacy/" target="_blank">webpage</a>
            </div>
            <div id="abstract_cvpr2024" class="abstract">Accurately predicting whether an image is private before sharing it online is difficult due to the vast variety of content and the subjective nature of privacy itself. In this paper, we evaluate privacy models that use objects extracted from an image to determine why the image is predicted as private. To explain the decision of these models, we use feature-attribution to identify and quantify which objects (and which of their features) are more relevant to privacy classification with respect to a reference input (i.e., no objects localised in an image) predicted as public. We show that the presence of the person category and its cardinality is the main factor for the privacy decision.</div>
            <div id="bib_cvpr2024" class="bib">@Article{Xompero2024CPVRW_XAI4CV,
  title = {Explaining models relating objects and privacy},
  author={Xompero, A. and Bontonou, M. and Arbona, J. and Benetos, E. and Cavallaro, A.},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision and Pattern Recognition Workshops},
  note = {The 3rd Explainable AI for Computer Vision (XAI4CV) Workshop},
  month="18" # JUN,
  year = {2024},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-arxiv-id="10.48550/arXiv.2405.01646" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="workshops" data-year="2023">
          <img src="../images/acvr2023.png" alt="ACVR 2023" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Affordance segmentation of hand-occluded containers from exocentric images</div>
            <div class="publication-venue">Eleventh International Workshop on Assistive Computer Vision and Robotics (ACVR), International Conference on Computer Vision (ICCV), Paris (France), 2 October 2023</div>
            <div class="publication-authors">T. Apicella, <b>A. Xompero</b>, E. Ragusa, R. Berta, A. Cavallaro, P. Gastaldo</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_acvr2023')">abstract</a>
              <a href="https://doi.org/10.1109/ICCVW60793.2023.00204" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2308.11233" target="_blank">arxiv</a>
              <a href="#" onclick="toggleBib(event, 'bib_acvr2023')">bibtex</a>
              <a href="https://apicis.github.io/projects/acanet.html" target="_blank">webpage</a>
              <a href="https://doi.org/10.5281/zenodo.5085800" target="_blank">data</a>
              <a href="https://github.com/SEAlab-unige/acanet" target="_blank">code</a>
            </div>
            <div id="abstract_acvr2023" class="abstract">Visual affordance segmentation identifies the surfaces of an object an agent can interact with. Common challenges for the identification of affordances are the variety of the geometry and physical properties of these surfaces as well as occlusions. In this paper, we focus on occlusions of an object that is hand-held by a person manipulating it. To address this challenge, we propose an affordance segmentation model that uses auxiliary branches to process the object and hand regions separately.</div>
            <div id="bib_acvr2023" class="bib">@Article{Apicella2023ICCVW,
  title = {Affordance segmentation of hand-occluded containers from exocentric images},
  author={Apicella, T. and Xompero, A., and Ragusa, E. and Berta, R. and Cavallaro, A. and Gastaldo, P.},
  booktitle = {Proceedings of the International Conference on Computer Vision Workshops},
  note = {International Workshop on Assistive Computer Vision and Robotics},
  month="2" # OCT,
  year = {2023},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1109/ICCVW60793.2023.00204" data-arxiv-id="10.48550/arXiv.2308.11233" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="workshops" data-year="2022">
          <img src="../images/xview_iwdsc22.svg" alt="IWDSC 2022" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Cross-Camera View-Overlap Recognition</div>
            <div class="publication-venue">International Workshop on Smart Distributed Cameras (IWDSC) at European Conference on Computer Vision (ECCV), October 2022</div>
            <div class="publication-authors"><b>A. Xompero</b>, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_iwdsc22')">abstract</a>
              <a href="https://doi.org/10.1007/978-3-031-25075-0_19" target="_blank">paper</a>
              <a href="https://doi.org/10.48550/arXiv.2208.11661" target="_blank">arxiv</a>
              <a href="#" onclick="toggleBib(event, 'bib_iwdsc22')">bibtex</a>
              <a href="research/xview" target="_blank">webpage</a>
              <a href="https://doi.org/10.5281/zenodo.7235890" target="_blank">data</a>
              <a href="https://github.com/kerolex/CrossCameraViewOverlapRecognition" target="_blank">code</a>
              <a href="https://youtu.be/MoFhI40Wd0s" target="_blank">video</a>
            </div>
            <div id="abstract_iwdsc22" class="abstract">We propose a decentralised view-overlap recognition framework that operates across freely moving cameras without the need of a reference 3D map. Each camera independently extracts, aggregates into a hierarchical structure, and shares feature-point descriptors over time. A view overlap is recognised by view-matching and geometric validation to discard wrongly matched views. The proposed framework is generic and can be used with different descriptors.</div>
            <div id="bib_iwdsc22" class="bib">@Article{Xompero2022IWDSC,
  title = {Cross-Camera View-Overlap Recognition},
  author = {Xompero, A. and Cavallaro, A.},
  booktitle = {Proceedings of the European Conference on Computer Vision Workshops},
  note = {International Workshop on Distributed Smart Cameras},
  month="24" # OCT,
  year = {2022},
}</div>
            <div class="publication-badge">
              <div data-badge-type="medium-donut" data-doi="10.1007/978-3-031-25075-0_19" data-arxiv-id="10.48550/arXiv.2208.11661" data-condensed="true" data-hide-no-mentions="true" data-hide-less-than="0" data-badge-popover="left" class="altmetric-embed"></div>
            </div>
          </div>
        </article>

        <!-- PREPRINTS -->
        <article class="publication-item" data-filter="preprints" data-year="2025">
          <img src="../images/vlms_privacy.png" alt="Zero-Shot Image Privacy" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Zero-Shot Image Privacy Classification with Vision-Language Models</div>
            <div class="publication-venue">September 2025</div>
            <div class="publication-authors">A. E. Baia, <b>A. Xompero</b>, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_baia25')">abstract</a>
              <a href="https://doi.org/10.48550/arXiv.2510.09253" target="_blank">arxiv</a>
              <a href="../resources/papers/Baia_Xompero_Cavallaro_PrivacyVLMs.pdf" target="_blank">paper</a>
              <a href="#" onclick="toggleBib(event, 'bib_baia25')">bibtex</a>
            </div>
            <div id="abstract_baia25" class="abstract">While specialized learning-based models have historically dominated image privacy prediction, the current literature increasingly favours adopting large Vision-Language Models (VLMs) designed for generic tasks. This trend risks overlooking the performance ceiling set by purpose-built models due to a lack of systematic evaluation. To address this problem, we establish a zero-shot benchmark for image privacy classification, enabling a fair comparison. We evaluate the top-3 open-source VLMs, according to a privacy benchmark, using task-aligned prompts and we contrast their performance, efficiency, and robustness against established vision-only and multi-modal methods.</div>
            <div id="bib_baia25" class="bib">@misc{Baia2025_VLM_Privacy,
  title = {Zero-Shot Image Privacy Classification with Vision-Language Models},
  author = {Baia, A. E. and Xompero, A. and Cavallaro, A.},
  year=2025
}</div>
          </div>
        </article>

        <article class="publication-item" data-filter="preprints" data-year="2025">
          <img src="../images/visaff_survey.svg" alt="Visual Affordances" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Visual Affordances: Enabling robots to understand object functionality</div>
            <div class="publication-venue">May 2025</div>
            <div class="publication-authors">T. Apicella, <b>A. Xompero</b>, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_apicella25arxiv')">abstract</a>
              <a href="https://doi.org/10.48550/arXiv.2505.05074" target="_blank">arxiv</a>
              <a href="#" onclick="toggleBib(event, 'bib_apicella25')">bibtex</a>
              <a href="https://apicis.github.io/aff-survey/" target="_blank">webpage</a>
              <a href="https://github.com/apicis/aff-survey" target="_blank">code</a>
            </div>
            <div id="abstract_apicella25arxiv" class="abstract">Human-robot interaction for assistive technologies relies on the prediction of affordances, which are the potential actions a robot can perform on objects. Predicting object affordances from visual perception is formulated differently for tasks such as grasping detection, affordance classification, affordance segmentation, and hand-object interaction synthesis. In this work, we highlight the reproducibility issue in these redefinitions, making comparative benchmarks unfair and unreliable. To address this problem, we propose a unified formulation for visual affordance prediction, provide a comprehensive and systematic review of previous works highlighting strengths and limitations of methods and datasets, and analyse what challenges reproducibility.</div>
            <div id="bib_apicella25" class="bib">@misc{Apicella2025,
  title = {Visual Affordances: Enabling robots to understand object functionality},
  author = {Apicella, T. and Xompero, A. and Cavallaro, A.},
  year=2025
}</div>
          </div>
        </article>

        <article class="publication-item" data-filter="preprints" data-year="2022">
          <img src="https://corsmal.github.io/images/pose/final_v4.png" alt="6D Pose Estimation" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">A Mixed-Reality Dataset for Category-level 6D Pose and Size Estimation of Hand-occluded Containers</div>
            <div class="publication-venue">arXiv, 2022</div>
            <div class="publication-authors">X. Weber, <b>A. Xompero</b>, A. Cavallaro</div>
            <div class="publication-links">
              <a href="#" onclick="toggleAbstract(event, 'abstract_weber22arxiv')">abstract</a>
              <a href="https://doi.org/10.48550/arXiv.2211.10470" target="_blank">arxiv</a>
              <a href="#" onclick="toggleBib(event, 'bib_preprintweber')">bibtex</a>
              <a href="https://corsmal.github.io/pose.html" target="_blank">webpage</a>
              <a href="https://doi.org/10.5281/zenodo.5085800" target="_blank">data</a>
            </div>
            <div id="abstract_weber22arxiv" class="abstract">Estimating the 6D pose and size of household containers is challenging due to large intra-class variations in the object properties, such as shape, size, appearance, and transparency. The task is made more difficult when these objects are held and manipulated by a person due to varying degrees of hand occlusions caused by the type of grasps and by the viewpoint of the camera observing the person holding the object. In this paper, we present a mixed-reality dataset of hand-occluded containers for category-level 6D object pose and size estimation.</div>
            <div id="bib_preprintweber" class="bib">@misc{Weber2023ArXiv,
  title = {A Mixed-Reality Dataset for Category-level 6D Pose and Size Estimation of Hand-occluded Containers},
  author = {Weber, X. and Xompero, A. and Cavallaro, A.},
  eprint={2211.10470},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2211.10470}
}</div>
          </div>
        </article>

        <!-- THESIS -->
        <article class="publication-item" data-filter="thesis" data-year="2020">
          <img src="../images/phdthesis_main.jpg" alt="PhD Thesis" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">Local features for view matching across independently moving cameras</div>
            <div class="publication-venue">PhD Thesis, Queen Mary University of London, 2020</div>
            <div class="publication-authors">Alessio Xompero</div>
            <div class="publication-links">
              <a href="https://qmro.qmul.ac.uk/xmlui/handle/123456789/68494" target="_blank">pdf</a>
            </div>
            <div class="thesis-info">
              <strong>Supervisors:</strong><br>Prof. Andrea Cavallaro, Queen Mary University of London, United Kingdom<br>Dr. Oswald Lanz, Fondazione Bruno Kessler, Italy<br><br>
              <strong>Examiners:</strong><br>Dr. Stefan Leutenegger, Imperial College London, United Kingdom<br>Dr. Jean-Yves Guillemaut, University of Surrey, United Kingdom
            </div>
          </div>
        </article>

        <article class="publication-item" data-filter="thesis" data-year="2015">
          <img src="../images/viprot_demo.gif" alt="Master Thesis" class="publication-image">
          <div class="publication-content">
            <div class="publication-title">ViProT: A Visual Probabilistic Model for Moving Interest Points Clusters Tracking</div>
            <div class="publication-venue">Master's Thesis, University of Trento, 2015</div>
            <div class="publication-authors">Alessio Xompero</div>
            <div class="publication-links">
              <a href="projects/MasterThesis_ViProT.pdf" target="_blank">pdf</a>
            </div>
            <div class="thesis-info">
              <strong>Advisors:</strong><br>Dr. Nicola Conci, University of Trento, Italy<br>Dr. <a href="https://team.inria.fr/robotlearn/team-members/radu-patrice-horaud/" target="_blank" style="color: #0066cc;">Radu Patrice Horaud</a>, INRIA Grenoble Rhne-Alpes, France<br>Dr. <a href="http://xavirema.eu/" target="_blank" style="color: #0066cc;">Xavier Alameda-Pineda</a>, INRIA Grenoble Rhne-Alpes, France<br>Dr. <a href="https://sites.google.com/site/sileyeoba/home" target="_blank" style="color: #0066cc;">Sileye Ba</a>, INRIA Grenoble Rhne-Alpes, France
            </div>
          </div>
        </article>
      </section>
    </div>
  </div>

  <footer class="site-footer">
    <div class="footer-col social-col" aria-label="Social media" style="text-align:center;">  
      <a href="https://orcid.org/0000-0002-8227-8529" aria-label="ORCID" rel="noopener noreferrer" target="_blank"><i class="ai ai-orcid ai-fw" aria-hidden="true"></i></a>
      <a href="https://scholar.google.com/citations?user=r7jxqOAAAAAJ&hl=en&oi=ao" aria-label="Google Scholar" rel="noopener noreferrer" target="_blank"><i class="ai ai-fw ai-google-scholar" aria-hidden="true"></i></a>
      <a href="https://github.com/kerolex/" aria-label="GitHub" rel="noopener noreferrer" target="_blank"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>       
      <a href="https://www.linkedin.com/in/alessioxompero/?locale=en_US" aria-label="LinkedIn" rel="noopener noreferrer" target="_blank"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      <a href="https://bsky.app/profile/axompi.bsky.social" target="_blank" aria-label="Bluesky" rel="noopener noreferrer"><i class="fa-brands fa-bluesky" aria-hidden="true"></i></a>
    </div>
    <p>
      Copyright &copy; 2015-<span id="currentYear"></span> - Alessio Xompero
    </p>
  </footer>

  <script>
    // Toggle abstract visibility
    function toggleAbstract(e, id) {
      e.preventDefault();
      const elem = document.getElementById(id);
      elem.classList.toggle('visible');
    }

    // Toggle bibtex visibility
    function toggleBib(e, id) {
      e.preventDefault();
      const elem = document.getElementById(id);
      elem.classList.toggle('visible');
    }

    // Filter functionality
    let currentCategoryFilter = 'all';
    let currentYearFilter = 'all';

    function attachCategoryFilterListeners() {
      const categorySection = document.querySelector('.filter-section');
      if (!categorySection) return;
      categorySection.querySelectorAll('.filter-btn').forEach(btn => {
        btn.addEventListener('click', function() {
          const filter = this.getAttribute('data-filter');
          categorySection.querySelectorAll('.filter-btn').forEach(b => {
            b.classList.remove('active');
          });
          this.classList.add('active');
          currentCategoryFilter = filter;
          filterPublications();
        });
      });
    }

    function attachYearFilterListeners() {
      const yearSection = document.getElementById('year-filters');
      if (!yearSection) return;
      yearSection.querySelectorAll('.filter-btn').forEach(btn => {
        btn.addEventListener('click', function() {
          const filter = this.getAttribute('data-filter');
          yearSection.querySelectorAll('.filter-btn').forEach(b => {
            b.classList.remove('active');
          });
          this.classList.add('active');
          currentYearFilter = filter;
          filterPublications();
        });
      });
    }

    function filterPublications() {
      document.querySelectorAll('.publication-item').forEach(item => {
        const itemCategory = item.getAttribute('data-filter');
        const itemYear = item.getAttribute('data-year');
        
        let show = true;
        
        if (currentCategoryFilter !== 'all' && itemCategory !== currentCategoryFilter) {
          show = false;
        }
        
        if (currentYearFilter !== 'all' && itemYear !== currentYearFilter) {
          show = false;
        }
        
        item.style.display = show ? 'block' : 'none';
      });
    }

    // Populate year filters
    function initYearFilters() {
      const yearFiltersContainer = document.getElementById('year-filters');
      if (yearFiltersContainer.children.length > 0) {
        return; // Already initialized
      }
      
      const years = new Set();
      document.querySelectorAll('.publication-item').forEach(item => {
        const year = item.getAttribute('data-year');
        if (year) years.add(year);
      });
      
      const sortedYears = Array.from(years).sort((a, b) => b - a);
      
      // Add "All Years" button
      const allBtn = document.createElement('button');
      allBtn.className = 'filter-btn active';
      allBtn.setAttribute('data-filter', 'all');
      allBtn.textContent = 'All Years';
      yearFiltersContainer.appendChild(allBtn);
      
      sortedYears.forEach(year => {
        const btn = document.createElement('button');
        btn.className = 'filter-btn';
        btn.setAttribute('data-filter', year);
        btn.textContent = year;
        yearFiltersContainer.appendChild(btn);
      });
      
      // Attach listeners to year buttons
      attachYearFilterListeners();
    }

    // Update current year in footer
    document.getElementById('currentYear').textContent = new Date().getFullYear();

    // Initialize
    function initializeFilters() {
      initYearFilters();
      attachCategoryFilterListeners();
      attachYearFilterListeners();
    }

    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', initializeFilters);
    } else {
      initializeFilters();
    }
  </script>
</body>

</html>
