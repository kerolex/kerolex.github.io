<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-364488019"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'G-364488019');
	</script>

	<title>Projects | Alessio Xompero, PhD</title>

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="Research projects I contributed to. Audio-visual sensing, robotics, AI, nuclear robotics, and multimodal AI.">
	<meta property="og:title" content="Research Projects | Alessio Xompero, PhD">
	<meta property="og:description" content="Research projects in computer vision, robotics, AI, and multimodal sensing.">
	<meta property="og:type" content="website">
	<meta name="image" property="og:image" content="../images/AlessioXompero.JPG">

	<link rel="shortcut icon" href="../images/profile_icon_ax.JPG" />
	<link rel="stylesheet" href="../css/mystyle.css">
	<link rel="stylesheet" href="../css/all.css">
	<link rel="stylesheet" href="../css/academicons.css">

	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css">

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&display=swap" rel="stylesheet">
	<link rel="dns-prefetch" href="https://www.googletagmanager.com">

	<script src="../js/menu.js" defer></script>
</head>

<body>
	<div id="menu">
		<div class="logo">
			<a href="../index.html" class="active"><strong>Alessio Xompero, Ph.D.</strong></a>			
		</div>

		<!-- From https://www.menucool.com/ddmenu/one-menu-for-all-pages -->
		<nav aria-label="Primary Navigation">
			<div class="menu-toggle"><span></span></div>
			<ul>			
				<li class="subnav">
					<a>Research</a>
					<div class="subnav-content">
						<a href="../research/publications.html">Publications</a>
						<a href="../research/data.html">Data</a>
						<a href="../research/software.html">Software</a>
						<a href="../research/projects.html">Projects</a>
						<a href="../research/talks.html">Talks</a>
					</div>
				</li>
				<li class="subnav">
					<a>Services</a>
					<div class="subnav-content">
						<a href="../services/reviewing.html">Reviewing</a>
						<a href="../services/openscience.html">Community</a>
					</div>
				</li>
				<li class="subnav">
					<a>Teaching</a>
					<div class="subnav-content">
						<a href="../teaching/teaching.html">Teaching</a>
					</div>
				</li>
				<li class="subnav">
					<a>News</a>
					<div class="subnav-content">
						<a href="../news.html">All news</a>
						<a href="../blog.html">Blog</a>
					</div>
				</li>
				<li class="subnav">
					<a>About</a>
					<div class="subnav-content">
						<a href="../bio.html">Biography</a>
						<a href="../gallery.html">Gallery</a>
					</div>
				</li>
			</ul>
		</nav>
	</div>

	<div id="about">
		<h2>Projects</h2>

		<main id="main-content">
		<section id="data-section">
			<!-- Intro Section -->
			<article class="intro-section">
				<p>
					<!-- I contributed to the following projects 
					through my Ph.D. and PostDoc at 
					Queen Mary University of London (QMUL), U.K. -->
					Research projects spanning audio-visual sensing, robotics, AI, and explainability. 
					Interdisciplinary collaborations across UK, Switzerland, France, Italy, and Korea,
					combining expertise from computer vision, signal processing, sensors, and AI. 
					Dates indicate my period of involvement.
				</p>
			</article>

			<!-- Projects Container (using data.css layout) -->
				<div class="datasets-container">

					<!-- PROJECT 1: Multi-Modal Foundational Models -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="https://via.placeholder.com/350x200?text=Multi-Modal+Foundational+Models" 
								 alt="Multi-Modal Foundational Models and AI Accelerators project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								Multi-Modal Foundational Models and AI Accelerators for Zero-shot Intelligent Surveillance System
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="software-category-tag">Active</span>
								<span class="software-category-tag secondary">Surveillance</span>
								<span class="software-category-tag secondary">Foundational Models</span>
								<span class="software-category-tag secondary">Vision-language Models</span>
								<span class="software-category-tag secondary">Computer Vision</span>
								<span class="project-status-badge active">Ongoing</span>
							</div>

							<div class="project-meta">
								March 2025 - October 2025
							</div>
							
							<p class="dataset-description">
								Zero-shot intelligent surveillance system leveraging multi-modal foundational models 
								and AI hardware accelerators for scalable, real-time video understanding and scene analysis.
							</p>
							
							<div class="dataset-links">
								<strong>Principal Investigator:</strong> Dr. Changjae Oh

								<details class="partner-details">
									<summary><i class="fas fa-users"></i> Collaborating Partners (3)</summary>
									<div class="partner-list">
										<p><strong>QMUL</strong> - Queen Mary University of London, UK</p>
										<p><strong>IC</strong> - Imperial College, UK</p>
										<p><strong>Innodep</strong> - Innodep, South Korea</p>
									</div>
								</details>
							</div>
						</div>
					</article>

					<!-- PROJECT 2: GraphNEx -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="https://graphnex.github.io/images/circles.jpg" 
								 alt="GraphNEx: Graph Neural Networks for Explainable AI project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								<a href="https://graphnex.github.io/" target="_blank">
									GraphNEx: Graph Neural Networks for Explainable Artificial Intelligence
								</a>
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="software-category-tag">Completed</span>
								<span class="software-category-tag secondary">Explainability</span>
								<span class="software-category-tag secondary">Graph Neural Networks</span>
								<span class="software-category-tag secondary">Privacy Protection</span>
								<span class="software-category-tag secondary">Multimodal AI</span>
								<span class="project-status-badge completed">Finished</span>
							</div>

							<div class="project-meta">
								January 2023 - December 2024
							</div>
							
							<p class="dataset-description">
								The project focuses on extrapolating semantic concepts and meaningful relationships from graphs 
								by using concepts and tools from graph signal processing and graph machine learning, while promoting 
								human interpretability. The graph-based framework will adaptively evolve the graphical knowledge base 
								and develop inherently explainable AI. Interdisciplinary project between Queen Mary University of London (UK),
								EPFL (Switzerland), and ENSL (France).
							</p>
							
							<div class="dataset-links">
								<strong>Principal Investigator:</strong> Prof. Andrea Cavallaro<br>
								<a href="https://graphnex.github.io" class="dataset-link" target="_blank">
									<i class="fas fa-globe"></i> Project Website
								</a>
							</div>
						</div>
					</article>

					<!-- PROJECT 3: CORSMAL -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="https://corsmal.github.io/images/publications_2.jpg" 
								 alt="CORSMAL: Collaborative Object Recognition and Shared Manipulation project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								<a href="https://corsmal.github.io/" target="_blank">
									CORSMAL: Collaborative Object Recognition, Shared Manipulation and Learning
								</a>
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="software-category-tag">Completed</span>
								<span class="software-category-tag secondary">Robotics</span>
								<span class="software-category-tag secondary">Multimodal Sensing</span>
								<span class="software-category-tag secondary">Object Recognition</span>
								<span class="software-category-tag secondary">Benchmarking</span>
								<span class="software-category-tag secondary">Human-robot collaboration</span>
								<span class="project-status-badge completed">Finished</span>
							</div>

							<div class="project-meta">
								October 2019 - December 2022
							</div>
							
							<p class="dataset-description">
								Enabling robots to operate in noisy and potentially ambiguous environments 
								in collaboration with humans by exploring the fusion of multiple sensing modalities (sound, 
								vision, touch) to accurately and robustly estimate the physical properties of objects a person intends 
								to hand over to the robot. Benchmarking robotics and perception algorithms for object recognition and manipulation
								in human-robot handover scenarios. Interdisciplinary project between Queen Mary University of London (UK),
								EPFL (Switzerland), and Sorbonne Univeristy (France).
							</p>
							
							<div class="dataset-links">
								<strong>Principal Investigator:</strong> Prof. Andrea Cavallaro<br>
								<a href="https://corsmal.github.io" class="dataset-link" target="_blank">
									<i class="fas fa-globe"></i> Project Website
								</a>
							</div>
						</div>
					</article>

					<!-- PROJECT 4: NCNR -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="../images/ncnr_logo.png" 
								 alt="National Centre for Nuclear Robotics project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								National Centre for Nuclear Robotics (NCNR)
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="software-category-tag">Active</span>
								<span class="software-category-tag secondary">Robotics</span>
								<span class="software-category-tag secondary">Nuclear Environment</span>
								<span class="software-category-tag secondary">Autonomous Systems</span>
								<span class="project-status-badge completed">Finished</span>
							</div>

							<div class="project-meta">
								April 2020 - December 2021
							</div>
							
							<p class="dataset-description">
								An interdisciplinary project that brings together a diverse consortium of experts in robotics, AI, 
								sensors, radiation and resilient embedded systems from 8 universities in the UK.  
								Addressing complex problems in high gamma environments, 
								where human entries are not possible at all, or in alpha-contaminated environments, where air-fed suited 
								human entries are possible, but engender significant secondary waste (contaminated suits), and reduced 
								worker capability.
							</p>

							<div class="dataset-links">
								<strong>Principal Investigators (QMUL):</strong> Prof. Kaspar Althoefer and Prof. Andrea Cavallaro<br>
								<a href="https://www.robotics.qmul.ac.uk/projects/ncnr-nuclear-robotics/" 
								   class="dataset-link" target="_blank">
									<i class="fas fa-globe"></i> Partner Project Website (no longer avaiilable)
								</a>
							</div>
						</div>
					</article>

					<!-- PROJECT 5: Audio-Visual Intelligent Sensing -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="https://kerolex.github.io/images/icassp17.png" 
								 alt="Audio-visual intelligent sensing project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								Audio-Visual Intelligent Sensing
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="software-category-tag">Completed</span>
								<span class="software-category-tag secondary">Audio-Visual</span>
								<span class="software-category-tag secondary">Multimodal sensing</span>
								<span class="software-category-tag secondary">3D perception</span>
								<span class="software-category-tag secondary">Computer vision</span>
								<span class="software-category-tag secondary">Distributed cameras</span>
								<span class="project-status-badge completed">Finished</span>
							</div>

							<div class="project-meta">
								September 2015 - September 2020
							</div>
							
							<p class="dataset-description">
								Interdisciplinary project to enable mobile audio-visual monitoring for smart interactive and reactive environments 
								by exploring methods for people tracking, activity recognition, acoustic scene analysis, behaviour analysis, 
								distant-speech recognition and understanding applied to individuals as well as groups in multi-camera 
								multi-microphone environments.
							</p>
							
							<div class="dataset-links">
								<strong>Collaboration:</strong> CIS (Queen Mary University of London, UK) &amp; TeV (Fondazione Bruno Kessler, Italy) 
							</div>
						</div>
					</article>

				</div><!-- End datasets-container -->
		
		</section>
		</main>
	</div>

<footer class="site-footer">
	<div class="footer-col social-col" aria-label="Social media" style="text-align:center;">  
		<a href="https://orcid.org/0000-0002-8227-8529" aria-label="ORCID" rel="noopener noreferrer" target="_blank"><i class="ai ai-orcid ai-fw" aria-hidden="true"></i></a>
		<a href="https://scholar.google.com/citations?user=r7jxqOAAAAAJ&hl=en&oi=ao" aria-label="Google Scholar" rel="noopener noreferrer" target="_blank"><i class="ai ai-fw ai-google-scholar" aria-hidden="true"></i></a>
		<a href="https://github.com/kerolex/" aria-label="GitHub" rel="noopener noreferrer" target="_blank" ><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>				
		<a href="https://www.linkedin.com/in/alessioxompero/?locale=en_US" aria-label="LinkedIn" rel="noopener noreferrer" target="_blank"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
		<a href="https://bsky.app/profile/axompi.bsky.social" target="_blank" aria-label="Bluesky" rel="noopener noreferrer"><i class="fa-brands fa-bluesky" aria-hidden="true"></i></a>
	</div>
	<p>
		Copyright &copy; 2015-<span id="currentYear"></span> - Alessio Xompero
	</p>
</footer>
<script>
	document.getElementById('currentYear').textContent = new Date().getFullYear();
</script>
</body>
</html>
