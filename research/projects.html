<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-364488019"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'G-364488019');
	</script>

	<title>Projects | Alessio Xompero, PhD</title>

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="Research projects I contributed to. Audio-visual sensing, robotics, AI, nuclear robotics, and multimodal AI.">
	<meta property="og:title" content="Research Projects | Alessio Xompero, PhD">
	<meta property="og:description" content="Research projects in computer vision, robotics, AI, and multimodal sensing.">
	<meta property="og:type" content="website">
	<meta name="image" property="og:image" content="../images/AlessioXompero.JPG">

	<link rel="shortcut icon" href="../images/profile_icon_ax.JPG" />
	<link rel="stylesheet" href="../css/mystyle.css">
	<link rel="stylesheet" href="../css/all.css">
	<link rel="stylesheet" href="../css/academicons.css">

	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css">

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&display=swap" rel="stylesheet">
	<link rel="dns-prefetch" href="https://www.googletagmanager.com">

	<script src="../js/menu.js" defer></script>
</head>

<body>
	<div id="menu">
		<div class="logo">
			<a href="../index.html" class="active"><strong>Alessio Xompero, Ph.D.</strong></a>			
		</div>

		<!-- From https://www.menucool.com/ddmenu/one-menu-for-all-pages -->
		<nav aria-label="Primary Navigation">
			<div class="menu-toggle"><span></span></div>
			<ul>			
				<li class="subnav">
					<a>Research</a>
					<div class="subnav-content">
						<a href="../research/publications.html">Publications</a>
						<a href="../research/data.html">Data</a>
						<a href="../research/software.html">Software</a>
						<a href="../research/projects.html">Projects</a>
						<a href="../research/talks.html">Talks</a>
					</div>
				</li>
				<li class="subnav">
					<a>Services</a>
					<div class="subnav-content">
						<a href="../services/reviewing.html">Reviewing</a>
						<a href="../services/openscience.html">Community</a>
					</div>
				</li>
				<li class="subnav">
					<a>Teaching</a>
					<div class="subnav-content">
						<a href="../teaching/teaching.html">Teaching</a>
					</div>
				</li>
				<li class="subnav">
					<a>News</a>
					<div class="subnav-content">
						<a href="../news.html">All news</a>
						<a href="../blog.html">Blog</a>
					</div>
				</li>
				<li class="subnav">
					<a>About</a>
					<div class="subnav-content">
						<a href="../bio.html">Biography</a>
						<a href="../gallery.html">Gallery</a>
					</div>
				</li>
			</ul>
		</nav>
	</div>

	<div id="about">
		<h2>Projects</h2>

		<main id="main-content">
		<section id="data-section">
			<!-- Intro Section -->
			<article class="intro-section">
				<p>
					I contributed to the following projects 
					through my Ph.D. and PostDoc at 
					Queen Mary University of London (QMUL), U.K.
				</p>
			</article>

			<!-- Projects Container (using data.css layout) -->
				<div class="datasets-container">

					<!-- PROJECT 1: Multi-Modal Foundational Models -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="https://via.placeholder.com/350x200?text=Multi-Modal+Foundational+Models" 
								 alt="Multi-Modal Foundational Models and AI Accelerators project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								Multi-Modal Foundational Models and AI Accelerators for Zero-shot Intelligent Surveillance System
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="project-category-tag">Active</span>
								<span class="project-category-tag secondary">Surveillance</span>
								<span class="project-category-tag secondary">Foundational Models</span>
								<span class="project-status-badge active">Ongoing</span>
							</div>

							<div class="project-meta">
								March 2025 - present
							</div>
							
							<p class="dataset-description">
								Zero-shot intelligent surveillance system leveraging multi-modal foundational models 
								and AI hardware accelerators for scalable, real-time video understanding and scene analysis.
							</p>
							
							<div class="dataset-links">
								<strong>Principal Investigator:</strong> Dr. Changjae Oh
							</div>
						</div>
					</article>

					<!-- PROJECT 2: GraphNEx -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="https://graphnex.github.io/images/circles.jpg" 
								 alt="GraphNEx: Graph Neural Networks for Explainable AI project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								<a href="https://graphnex.github.io/" target="_blank">
									GraphNEx: Graph Neural Networks for Explainable Artificial Intelligence
								</a>
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="project-category-tag">Completed</span>
								<span class="project-category-tag secondary">Graph Learning</span>
								<span class="project-category-tag secondary">Explainability</span>
								<span class="project-status-badge completed">Finished</span>
							</div>

							<div class="project-meta">
								January 2023 - December 2024
							</div>
							
							<p class="dataset-description">
								The project focuses on extrapolating semantic concepts and meaningful relationships from graphs 
								by using concepts and tools from graph signal processing and graph machine learning, while promoting 
								human interpretability. The graph-based framework will adaptively evolve the graphical knowledge base 
								and develop inherently explainable AI.
							</p>
							
							<div class="dataset-links">
								<strong>Principal Investigator:</strong> Prof. Andrea Cavallaro<br>
								<a href="https://graphnex.github.io" class="dataset-link" target="_blank">
									<i class="fas fa-globe"></i> Project Website
								</a>
							</div>
						</div>
					</article>

					<!-- PROJECT 3: CORSMAL -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="https://corsmal.github.io/images/publications_2.jpg" 
								 alt="CORSMAL: Collaborative Object Recognition and Shared Manipulation project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								<a href="https://corsmal.github.io/" target="_blank">
									CORSMAL: Collaborative Object Recognition, Shared Manipulation and Learning
								</a>
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="project-category-tag">Completed</span>
								<span class="project-category-tag secondary">Robotics</span>
								<span class="project-category-tag secondary">Multimodal Sensing</span>
								<span class="project-category-tag secondary">Object Recognition</span>
								<span class="project-status-badge completed">Finished</span>
							</div>

							<div class="project-meta">
								October 2019 - December 2022
							</div>
							
							<p class="dataset-description">
								The project focuses on enabling robots to operate in noisy and potentially ambiguous environments 
								in collaboration with humans. The project explores the fusion of multiple sensing modalities (sound, 
								vision, touch) to accurately and robustly estimate the physical properties of objects a person intends 
								to hand over to a robot.
							</p>
							
							<div class="dataset-links">
								<strong>Principal Investigator:</strong> Prof. Andrea Cavallaro<br>
								<a href="https://corsmal.github.io" class="dataset-link" target="_blank">
									<i class="fas fa-globe"></i> Project Website
								</a>
							</div>
						</div>
					</article>

					<!-- PROJECT 4: NCNR -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="../images/ncnr_logo.png" 
								 alt="National Centre for Nuclear Robotics project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								National Centre for Nuclear Robotics (NCNR)
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="project-category-tag">Completed</span>
								<span class="project-category-tag secondary">Robotics</span>
								<span class="project-category-tag secondary">Nuclear Environment</span>
								<span class="project-category-tag secondary">Autonomous Systems</span>
								<span class="project-status-badge completed">Finished</span>
							</div>

							<div class="project-meta">
								April 2020 - December 2021
							</div>
							
							<p class="dataset-description">
								An interdisciplinary project that brings together a diverse consortium of experts in robotics, AI, 
								sensors, radiation and resilient embedded systems, to address complex problems in high gamma environments, 
								where human entries are not possible at all, or in alpha-contaminated environments, where air-fed suited 
								human entries are possible, but engender significant secondary waste (contaminated suits), and reduced 
								worker capability.
							</p>
							
							<div class="dataset-links">
								<a href="https://www.robotics.qmul.ac.uk/projects/ncnr-nuclear-robotics/" 
								   class="dataset-link" target="_blank">
									<i class="fas fa-external-link-alt"></i> Project Details
								</a>
							</div>
						</div>
					</article>

					<!-- PROJECT 5: Audio-Visual Intelligent Sensing -->
					<article class="dataset-card">
						<div class="dataset-media">
							<img src="https://kerolex.github.io/images/icassp17.png" 
								 alt="Audio-visual intelligent sensing project thumbnail">
						</div>
						
						<div class="dataset-content">
							<h3 class="dataset-title">
								Audio-Visual Intelligent Sensing
							</h3>
							
							<div style="margin-bottom: 0.8rem;">
								<span class="project-category-tag">Completed</span>
								<span class="project-category-tag secondary">Audio-Visual</span>
								<span class="project-category-tag secondary">Multimodal</span>
								<span class="project-category-tag secondary">Activity Recognition</span>
								<span class="project-status-badge completed">Finished</span>
							</div>

							<div class="project-meta">
								September 2015 - September 2020
							</div>
							
							<p class="dataset-description">
								Interdisciplinary project between the Centre for Intelligent Sensing (QMUL) and the Centre for 
								Information Technology at the Fondazione Bruno Kessler (FBK), Trento, Italy. The project focuses 
								on mobile audio-visual monitoring for smart interactive and reactive environments. The project explores 
								methods for people tracking, activity recognition, acoustic scene analysis, behaviour analysis, 
								distant-speech recognition and understanding applied to individuals as well as groups in multi-camera 
								multi-microphone environments.
							</p>
							
							<div class="dataset-links">
								<strong>Collaboration:</strong> Centre for Intelligent Sensing (QMUL) &amp; TeV (FBK) 
							</div>
						</div>
					</article>

				</div><!-- End datasets-container -->
		
		</section>
		</main>


		<!-- <table class="tg" width="95%">
			<tr>
				<td class="tg-km2t" style="vertical-align: top;">
					<a href="" TARGET = "_blank">
						<img src="" alt="" width="100%"/><br>
						</a>
				</td>
				<td class="tg-zv4m" style="vertical-align: top;">
						<b style="font-weight: bold;">Multi-Modal Foundational Models and AI Accelerators for Zero-shot Intelligent Surveillance System</b><br>
						March 2025 - present (<i>Active</i>)<br><br>
						Principal Investigator: Dr. Changjae Oh
				</td>
			</tr>
				<tr>
					<td class="tg-km2t" style="vertical-align: top;">
						<a href="https://graphnex.github.io/" TARGET = "_blank">
							<img src="https://graphnex.github.io/images/circles.jpg" alt="" width="100%"/><br>
							</a>
					</td>
					<td class="tg-zv4m" style="vertical-align: top;">
							<b style="font-weight: bold;">GraphNEx: Graph Neural Networks for Explainable Artificial Intelligence</b><br>
							January 2023 - December 2024 (<i>Completed</i>)<br><br>
							The project focuses on extrapolating semantic concepts and meaningful relationships from graphs by using concepts and tools from graph signal processing and graph machine learning, while promoting human interpretability. The graph-based framework will adaptively evolve the graphical knowledge base and develop inherently explainable AI.<br><br>
							Principal Investigator: Prof. Andrea Cavallaro<br>
							<a href="https://graphnex.github.io"><u>https://graphnex.github.io</u></a>
					</td>
				</tr>
				<tr>
					<td class="tg-km2t" style="vertical-align: top;">
						<a href="https://corsmal.github.io/" TARGET = "_blank">
							<img src="https://corsmal.github.io/images/publications_2.jpg" alt="" width="100%"/><br>
							</a>
					</td>
					<td class="tg-zv4m" style="vertical-align: top;">
							<b style="font-weight: bold;">CORSMAL: Collaborative Object Recognition, Shared Manipulation and Learning</b><br>
							October 2019 - December 2022 (<i>Completed</i>)<br><br>
							The project focuses on enabling robots to operate in noisy and potetially ambiguous environments in collaboration with humans. The project explores the fusion of multiple sensing modalities (sound, vision, touch) to accurately and robustly estimate the physical properties of objects a person intends to hand over to a robot.<br><br>
							Principal Investigator: Prof. Andrea Cavallaro<br>
							<a href="https://corsmal.github.io"><u>https://corsmal.github.io</u></a>
					</td>
				</tr>
				<tr>
					<td class="tg-km2t" style="vertical-align: middle;">
						<a href="" TARGET = "_blank">
							<img src="../images/ncnr_logo.png" alt="" width="100%"/><br>
							</a>
					</td>
					<td class="tg-zv4m" style="vertical-align: top;">
							<b style="font-weight: bold;">National Centre for Nuclear Robotics (NCNR)</b><br>
							April 2020 - December 2021 (<i>Completed</i>)<br><br>
							An interdisciplanary project that brings together a diverse consortium of experts in robotics, AI, sensors, radiation and resilient embedded systems, to address complex problems in high gamma environments, where human entries are not possible at all, or in alpha-contaminated environments, where air-fed suited human entries are possible, but engender significant secondary waste (contaminated suits), and reduced worker capability.<br><br>
							<a href="https://www.robotics.qmul.ac.uk/projects/ncnr-nuclear-robotics/"><u>https://www.robotics.qmul.ac.uk/projects/ncnr-nuclear-robotics/</u></a>
					</td>
				</tr>
				<tr>
					<td class="tg-km2t" style="vertical-align: top;">
						<a href="" TARGET = "_blank">
							<img src="https://kerolex.github.io/images/icassp17.png" alt="" width="100%"/><br>
							</a>
					</td>
					<td class="tg-zv4m" style="vertical-align: top;">
							<b style="font-weight: bold;">Audio-visual intelligent sensing</b><br>
							September 2015 - September 2020 (<i>Completed</i>)<br><br>
							Interdisciplinary project between the Centre for Intelligent Sensing (QMUL) and the Centre for Information Technology (http://ict.fbk.eu) at the Fondazione Bruno Kessler (FBK), Trento, Italy. The project focuses on mobile audio-visual monitoring for smart interactive and reactive environments. The project explores methods for people tracking, activity recognition, acoustic scene analysis, behaviour analysis, distant-speech recognition and understanding applied to individuals as well as groups in multi-camera multi-microphone environments.<br><br>
					</td>
				</tr>
			</table> -->
	</div>

<footer class="site-footer">
	<div class="footer-col social-col" aria-label="Social media" style="text-align:center;">  
		<a href="https://orcid.org/0000-0002-8227-8529" aria-label="ORCID" rel="noopener noreferrer" target="_blank"><i class="ai ai-orcid ai-fw" aria-hidden="true"></i></a>
		<a href="https://scholar.google.com/citations?user=r7jxqOAAAAAJ&hl=en&oi=ao" aria-label="Google Scholar" rel="noopener noreferrer" target="_blank"><i class="ai ai-fw ai-google-scholar" aria-hidden="true"></i></a>
		<a href="https://github.com/kerolex/" aria-label="GitHub" rel="noopener noreferrer" target="_blank" ><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>				
		<a href="https://www.linkedin.com/in/alessioxompero/?locale=en_US" aria-label="LinkedIn" rel="noopener noreferrer" target="_blank"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
		<a href="https://bsky.app/profile/axompi.bsky.social" target="_blank" aria-label="Bluesky" rel="noopener noreferrer"><i class="fa-brands fa-bluesky" aria-hidden="true"></i></a>
	</div>
	<p>
		Copyright &copy; 2015-<span id="currentYear"></span> - Alessio Xompero
	</p>
</footer>
<script>
	document.getElementById('currentYear').textContent = new Date().getFullYear();
</script>
</body>
</html>
